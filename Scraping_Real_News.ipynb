{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Relevant Libraries\n",
        "\n",
        "Before scraping real news, it is important to import a couple of libraried that will enable us to scrape and organize the content that will be obtained from the sites chosen - in this case, GMA and Rappler."
      ],
      "metadata": {
        "id": "ltGXdnoAJkqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Libraries\n",
        "We first start by importing libraries that are not necessarily used for scraping, but would enable us to do the task better.\n",
        "\n",
        "*  `time`: provides functions for handling time related tasks ([Programiz](https://www.programiz.com/python-programming/time), n.d.)\n",
        "> this allows us to enable pauses for the loops that we will run later on\n",
        "\n",
        "*   `pandas` : has functions for data analysis and manipulation ([pandas](https://pandas.pydata.org/), n.d.)\n",
        "> this allows us to obtain all scraped data and organize them in a format that can be used for EDA and to create the model later on\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s-a45YrMKlE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "sP_GeHWhKrpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selenium\n",
        "> The following libraries are needed to scrape GMA and Rappler. Selenium is used since we navigate through multiple pages in both sites.\n",
        "\n",
        "*    `from selenium import webdriver` enables automation of web browser\n",
        "*    `from selenium.webdriver.common.keys import Keys` enables simulation of keyboard keys\n",
        "*   `from selenium.webdriver.common.by import By` helps with locating elements in the webpage through XPath\n",
        "*   `from selenium.webdriver.chrome.options import Options` enables us to specify certain preferences when initializing the WebDriver, aids in preventing multiple windows from initializing when performing the loops"
      ],
      "metadata": {
        "id": "Yz_1GVS2MHH8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWwceB84JjQl"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation for Scraping\n",
        "Since the needed libraries have already been imported, scraping can now be started. However, preliminaries should be done first. The following variables are needed:\n",
        "\n",
        "* driver_path : contains the path where the WebDriver is found\n",
        "* chrome_options : added to skip having displays / new windows while the code is running\n",
        "\n",
        "Additionally, the user has to specify the download and specify the driver path to be able to run subsequent codes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MepOlkUMMXdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part necessitates the user to specify their own driver path that will enable scraping through initializing the Chrome WebDriver."
      ],
      "metadata": {
        "id": "sAdcgkMANgjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# insert your own driver path below:\n",
        "driver_path=\"/Users/beatricebanzon/Desktop/dlsu/col/2/T3 22.23/DATA102/chromedriver_mac_arm64/chromedriver.exe\""
      ],
      "metadata": {
        "id": "X-3AR-bmMcSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned earlier, the Options function is used to specify preferences. In this case, we specify opting to disable opening a visible GUI each time the subsequent loops are ran."
      ],
      "metadata": {
        "id": "Jnpth1-DOLW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless=new\")\n",
        "driver=webdriver.Chrome(driver_path, chrome_options=chrome_options)"
      ],
      "metadata": {
        "id": "tIDhJqAjnYrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other variables shall be specified, with the following purposes:\n",
        "* pause : number of delay in seconds\n",
        "* checker : will be used for the following loops to run"
      ],
      "metadata": {
        "id": "aLA9k_QHOYyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pause = 3\n",
        "checker = True"
      ],
      "metadata": {
        "id": "ebAWDw2knZQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping GMA"
      ],
      "metadata": {
        "id": "xhPQv06wMRrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GMA Pages\n",
        "> The pages containing the links of GMA's articles will be scraped.\n",
        "\n",
        "* gma_url : url of GMA containing the articles\n",
        "* counter : used to limit the number of scraped pages\n",
        "* gma_master : container for the links\n",
        "\n"
      ],
      "metadata": {
        "id": "hIzYtfjZM7ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The url below will be scraped - notably, it needs scrolling to load subsequent pages. In the said page, all the articles may be seen; thus, we start by getting the links of each article first."
      ],
      "metadata": {
        "id": "bbUGYqzzthgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gma_url=f\"https://www.gmanetwork.com/news/archives/topstories/\"\n",
        "counter = 1\n",
        "gma_master = []"
      ],
      "metadata": {
        "id": "Izen0VCRM9Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code, XPath is used to locate and obtain the elements from the\n",
        "HTML page. The loop below will be ran as the site scrolls.\n",
        "\n",
        "* '.find_elements' : for locating multiple elements (in this case, articles)\n",
        "* '.get_attribute' : for retrieving a specific attribute inside the element (in this case, for getting the link found in each element)\n"
      ],
      "metadata": {
        "id": "kcqcp34PPqIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver.get(gma_url)"
      ],
      "metadata": {
        "id": "Px62B5tZnWbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while checker:\n",
        "    url = driver.current_url\n",
        "    print (gma_url)\n",
        "\n",
        "    time.sleep(pause)\n",
        "\n",
        "    # the ul contains all the stories or articles found in each page\n",
        "    temp = driver.find_elements(By.XPATH, '//ul[@id=\"grid_thumbnail_stories\"]')\n",
        "\n",
        "    # all those with tag a under the previous ul with the specified class contains the href\n",
        "    for each in temp[0].find_elements(By.XPATH, '//a[@class=\"story_link story\"]'):\n",
        "        link = each.get_attribute(\"href\")\n",
        "        if link not in gma_master:\n",
        "            gma_master.append(link)\n",
        "\n",
        "    # the GMA site needs scrolling to load the following pages\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    time.sleep(5)\n",
        "\n",
        "    counter+=1\n",
        "\n",
        "    if counter==1001:\n",
        "        break"
      ],
      "metadata": {
        "id": "O1s4PNLCNBGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GMA Articles\n",
        "The links obtained in the previous code have been appended to gma_master. Thus, in this part, each of those links will be scraped to obtain the content needed."
      ],
      "metadata": {
        "id": "9PSkIvv3M1Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the DataFrame with the needed content\n",
        "gma_df = pd.DataFrame(columns=['Link', 'Author','Content'])"
      ],
      "metadata": {
        "id": "oSG53xRMMa1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loop below is used to get all the necessary information out of each article. Since some information has different XPaths per article, we make use of the try and except functions. All the obtained information is then transferred to the dataframe named gma_df which will then be used in data pre-processing."
      ],
      "metadata": {
        "id": "BQ-PwDiMuLv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to know how many articles have already been scraped\n",
        "scraped_count = 0\n",
        "\n",
        "for link in gma_master:\n",
        "    print (scraped_count, link)\n",
        "    driver.get(link)\n",
        "\n",
        "    time.sleep (pause)\n",
        "\n",
        "    checker = True\n",
        "\n",
        "    # to obtain the actual content of each article, we create a container for the paragraphs\n",
        "    paragraphs = []\n",
        "\n",
        "    # different articles have different XPaths for the whole content of the article\n",
        "    while checker:\n",
        "        try:\n",
        "            content = driver.find_element(By.XPATH, '//div[@class=\"story_main\"]')\n",
        "        except:\n",
        "            content = driver.find_element(By.XPATH, '//div[@class=\"article-body\"]')\n",
        "\n",
        "    # the following codes are needed to get only the text of those with tag p directly under content\n",
        "        pars = content.find_elements(By.XPATH, 'p')\n",
        "        for par in pars:\n",
        "            text = par.text.strip()\n",
        "            if text:\n",
        "                paragraphs.append(text)\n",
        "    # since there are several paragraphs in one article, there is a need to append\n",
        "        concat_pars = ' '.join(paragraphs)\n",
        "\n",
        "    # to get the author:\n",
        "        try:\n",
        "            author_elements = content.find_elements(By.XPATH, '//div[@class=\"main-byline\"]')\n",
        "            author = [element.text for element in author_elements]\n",
        "        except:\n",
        "            try:\n",
        "                author_elements = content.find_elements(By.XPATH, '//div[@class=\"article-author\"]')\n",
        "                author = [element.text for element in author_elements]\n",
        "            except:\n",
        "                author = [\"Author not found\"]\n",
        "\n",
        "        checker = False\n",
        "        scraped_count += 1\n",
        "\n",
        "    # appending all the scraped content into the DataFrame\n",
        "    gma_df = gma_df.append({'Link': link,'Author':author, 'Content': concat_pars}, ignore_index=True)\n",
        "    # storing everything into a CSV file to prevent loss\n",
        "    gma_df.to_csv('gma_dataframe.csv', index=False)"
      ],
      "metadata": {
        "id": "jqlze-KNMomM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Rappler\n",
        "\n",
        "The method used in scraping GMA is done with Rappler. Similarly, we obtain the links first before scraping each article."
      ],
      "metadata": {
        "id": "g_iJDbtlNbTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rappler Pages\n",
        "> In scraping Rappler's pages, there was a need to change the last part of the URL instead of scrolling (which was done on GMA's). Through the loop below, we are able to navugate through Rappler's different pages and use the appropriate XPaths after checking the site's HTML."
      ],
      "metadata": {
        "id": "k9K0oMVvNd4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following variables will be used:\n",
        "\n",
        "* rap_master : container for the links\n",
        "* checker : will be used for the following loops to run\n",
        "* counter : used to limit the number of scraped pages"
      ],
      "metadata": {
        "id": "FabXQG4PPVBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rap_master = []\n",
        "checker = True\n",
        "counter = 1"
      ],
      "metadata": {
        "id": "xKTcSGPSNlvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following functions will be used to locate the links:\n",
        "* '.find_elements' : for locating multiple elements (in this case, articles)\n",
        "* '.get_attribute' : for retrieving a specific attribute inside the element (in this case, for getting the link found in each element)"
      ],
      "metadata": {
        "id": "fzyubl7RPtKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while checker:\n",
        "    url_rap=f\"https://www.rappler.com/latest/page/{counter}/\"\n",
        "    print ('Scraping', url_rap)\n",
        "    driver.get(url_rap)\n",
        "\n",
        "    time.sleep(pause)\n",
        "\n",
        "    # contains all the articles\n",
        "    temp = driver.find_elements(By.XPATH, '//main[@id=\"primary\"]')\n",
        "\n",
        "    # to get all the links in the current page\n",
        "    for each in temp[0].find_elements(By.XPATH, '//article//h2//a'):\n",
        "        link = each.get_attribute(\"href\")\n",
        "        if link not in rap_master:\n",
        "            rap_master.append(link)\n",
        "\n",
        "    time.sleep(pause)\n",
        "    # necessary to change the url\n",
        "    counter+=1\n",
        "\n",
        "    if counter==1200:\n",
        "        break"
      ],
      "metadata": {
        "id": "7ARVatQANk_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rappler Articles"
      ],
      "metadata": {
        "id": "Z71CmgUFNfQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by  reating the DataFrame that will contain all the content from each article from Rappler."
      ],
      "metadata": {
        "id": "cCjdspMlP4Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rap_df = pd.DataFrame(columns=['Link', 'Author','Content'])"
      ],
      "metadata": {
        "id": "qz7WOdmdSlDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loop below is used to get all the necessary information out of each article. Since some information has different XPaths per article, we make use of the try and except functions. All the obtained information is then transferred to the dataframe named rap_df which will then be used in data pre-processing."
      ],
      "metadata": {
        "id": "HCYBxfBJvEED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to know how many articles have already been scraped\n",
        "scraped_count = 0\n",
        "\n",
        "for link in rap_master:\n",
        "    print (scraped_count, link)\n",
        "    driver.get(link)\n",
        "\n",
        "    time.sleep (pause)\n",
        "\n",
        "    checker = True\n",
        "    paragraphs = []\n",
        "    while checker:\n",
        "        # for main content\n",
        "        content = driver.find_element(By.XPATH, '//div[@class=\"post-single__content entry-content\"]')\n",
        "        pars = content.find_elements(By.XPATH, 'p')\n",
        "        for par in pars:\n",
        "            text = par.text.strip()\n",
        "            if text:\n",
        "                paragraphs.append(text)\n",
        "        # again, all the paragraphs have to be appended since not all the paragraphs are contained in a single tag\n",
        "        concat_pars = ' '.join(paragraphs)\n",
        "        checker = False\n",
        "\n",
        "        # to obtain the author, the following XPath is used:\n",
        "        try:\n",
        "            author_elements = content.find_elements(By.XPATH, '//a[@class=\"post-single__author\"]')\n",
        "            author = [element.text for element in author_elements]\n",
        "        except:\n",
        "            author = [\"Author not found\"]\n",
        "\n",
        "        scraped_count += 1\n",
        "\n",
        "        # appending all the obtained content to the DataFrame\n",
        "    rap_df = rap_df.append({'Link': link,'Author':author, 'Content': concat_pars}, ignore_index=True)\n",
        "        # storing to prevent loss\n",
        "    rap_df.to_csv('rap_dataframe.csv', index=False)"
      ],
      "metadata": {
        "id": "mr_1rY5-NcMx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}