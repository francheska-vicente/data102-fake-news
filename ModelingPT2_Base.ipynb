{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c1ebb4",
   "metadata": {},
   "source": [
    "# Insert Title Here\n",
    "**DATA102 S11 Group 3*\n",
    "- Banzon, Beatrice Elaine B.\n",
    "- Buitre, Cameron\n",
    "- Marcelo, Andrea Jean C.\n",
    "- Navarro, Alyssa Riantha R.\n",
    "- Vicente, Francheska Josefa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0aebb6",
   "metadata": {},
   "source": [
    "# **Requirements and Imports**\n",
    "Before starting, the relevant libraries and files in building and training the model should be loaded into the notebook first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3cc136",
   "metadata": {},
   "source": [
    "## **Basic** Libraries\n",
    "* `numpy` contains a large collection of mathematical functions\n",
    "* `pandas` contains functions that are designed for data manipulation and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca2cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8bcdf0",
   "metadata": {},
   "source": [
    "## **`Natural Language Processing`** Libraries\n",
    "* `train_test_split` is a function that allows the dataset to be split into two randomly.\n",
    "* `TFidfVectorizer` converts the given text documents into a matrix, which has TF-IDF features\n",
    "* `CountVectorizer` converts the given text documents into a matrix, which has the counts of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb97206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a18b7",
   "metadata": {},
   "source": [
    "## **`Machine Learning`** Libraries\n",
    "The following classes are classifiers that implement different methods of classification.\n",
    "* `LogisticRegression` is a class under the linear models module that implements regularized logistic regression\n",
    "* `MultinomialNB` is a class under the Naive Bayes module that allows the classification of discrete features\n",
    "* `RandomForestClassifier` is a class under the ensemble module that trains by fitting using a number of decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf0cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6531f7c",
   "metadata": {},
   "source": [
    "On the other hand, these classes computes and visualizes the different scores about how well a model works.\n",
    "* `f1_score` computes the balanced F-score by comparing the actual classes and the predicted classes\n",
    "* `hamming_loss` computes the fraction of labels that were incorrectly labeled by the model\n",
    "* `accuracy_score` computes the accuracy by determining how many classes were correctly predicted\n",
    "* `precision_recall_fscore_support`computes the precision, recall, F-measure and support per class\n",
    "* `ConfusionMatrixDisplay` allows the visualization of the computed confusion matrix\n",
    "* `confusion_matrix`  is a function that displays the number of samples that are correctly and incorrectly labeled by the model, by grouping them into four groups (i.e., True Positives, False Positives, True Negatives, False Negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ba854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, hamming_loss, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd3c63",
   "metadata": {},
   "source": [
    "Meanwhile, `GridSearchCV` is a cross-validation class that allows the exhaustive search over all possible combinations of hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7701e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d53126-f75e-4ce5-a264-b1360efd127f",
   "metadata": {},
   "source": [
    "Next, `ELI5` is a python library that holds support for machine learning algorithms frameworks and visualize different machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e0e87-a779-4761-9833-90e023ac8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbfb372",
   "metadata": {},
   "source": [
    "Last, `pickle` is a module that can serialize and deserialize objects. In this notebook, it is used to save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e14814",
   "metadata": {},
   "source": [
    "### Datasets and Files\n",
    "To train the models that utilizes the traditional machine learning algorithms, the dataset that was cleaned with the removal of unnecessary sequences will be loaded using the [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb7b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('cleaned_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21732b0e",
   "metadata": {},
   "source": [
    "# **Feature Engineering**\n",
    "\n",
    "As we cannot directly feed the text data as input to the machine learning models, we have to convert it into the format that they can understandâ€”numbers. Before doing that, since we want to save the models and vectorizers that we will be using, we will first need to define the values and functions to do so, starting with the folder where we will be saving it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = './saved_models/Trad_ML/vectorizers/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d621ea",
   "metadata": {},
   "source": [
    "Next, we will be creating a function that will be saving the vectorizer to the specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vectorizers (vectorizer, vectorizer_name):\n",
    "    vectorizer_filename = main_directory + vectorizer_name + '.pkl'\n",
    "    \n",
    "    with open(vectorizer_filename, 'wb') as file:\n",
    "        pickle.dump(vectorizer, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37fb5e",
   "metadata": {},
   "source": [
    "## Splitting the Dataset into **`Train`**, **`Validation`**, and **`Test`** Split\n",
    "Let us first define the **X** (input) and **y** (target/output) of our model. This is done to allow the stratifying of the data when it is split into the train, val and test.\n",
    "\n",
    "The **X** (input) can be retrieved by getting the `text` column in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465abc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df ['text']\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76697bd0",
   "metadata": {},
   "source": [
    "Meanwhile, the **y** value (i.e., the value that we would be \"feeding\" our models) is the `class` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df ['class']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89690df",
   "metadata": {},
   "source": [
    "Now that we have declared the input and the target output of our models, we can use the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to divide the dataset into two splits. Some things to note are: (1) the split is stratified based on the **y values**, (2) the value of the random state was set to 42 for reproducibility, and (3) the dataset is shuffled.\n",
    "\n",
    "First, let us create the train and test set. The test set is made up of 20% of the original dataset, which infers that the second split is 80% of the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eedd5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state = 42, \n",
    "                                                    shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69737817",
   "metadata": {},
   "source": [
    "Second, we will be splitting the remaining 80% of the original dataset into two: the train and val sets. The train set will be 90% of the second split, while the val set will be 10% of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                  y_train, \n",
    "                                                  test_size = 0.1,\n",
    "                                                  stratify = y_train,\n",
    "                                                  random_state = 42, \n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445751c",
   "metadata": {},
   "source": [
    "To check if the shapes of the input and output are the same, we will be looking at the shapes of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train')\n",
    "print('Input  shape: ', X_train.shape)\n",
    "print('Output shape: ', y_train.shape, '\\n')\n",
    "\n",
    "print('Val')\n",
    "print('Input  shape: ', X_val.shape)\n",
    "print('Output shape: ', y_val.shape, '\\n')\n",
    "\n",
    "print('Test')\n",
    "print('Input  shape: ', X_test.shape)\n",
    "print('Output shape: ', y_test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ce9ccb",
   "metadata": {},
   "source": [
    "## Tokenizing with **`TF-IDF` Vectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94a74f",
   "metadata": {},
   "source": [
    "Now, we can proceed with tokenizing our input. To do this, we first create an instance of a [`TF-IDF Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) with default values for its parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8ec72",
   "metadata": {},
   "source": [
    "### **`Train`** Data\n",
    "With the created vectorizer, we can now use the [`fit_transform`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit_transform) function, which will learn the vocabulary and the inverse document frequency from the data provided, and then create a document-term matrix using the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b6ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dcd652",
   "metadata": {},
   "source": [
    "To use this vectorizer that has learned from the vocabulary, let us save it using the function we previously defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd196d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vectorizers(tfidf_vectorizer, 'tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c187fea",
   "metadata": {},
   "source": [
    "### **`Validation`** Data\n",
    "Using the [`transform`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.transform) function, we will be creating a document-term matrix for the validation set. For this, it is important to convert the datatype of the values in the validation set into **Unicode**, as this is the type accepted by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_val = tfidf_vectorizer.transform(X_val.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd689f",
   "metadata": {},
   "source": [
    "### **`Test`** Data\n",
    "Next, we will also [`transform`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.transform) our test data into a document-term matrix, and to do this, we also have to convert it into the **Unicode** datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bcfa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = tfidf_vectorizer.transform(X_test.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2561bb",
   "metadata": {},
   "source": [
    "## Tokenizing with **`Count` Vectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf2db6",
   "metadata": {},
   "source": [
    "We create a `CountVectorizer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ada95",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2fa7cb",
   "metadata": {},
   "source": [
    "### **`Train`** Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = count_vectorizer.fit_transform(X_train.values.astype('U'))\n",
    "save_vectorizers(count_vectorizer, 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673ab37",
   "metadata": {},
   "source": [
    "### **`Validation`** Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334cdeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_val = count_vectorizer.transform(X_val.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819764f",
   "metadata": {},
   "source": [
    "### **`Test`** Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b897f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_test = count_vectorizer.transform(X_test.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a9ae7f",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation\n",
    "\n",
    "Now that we have transformed our data into the format that our algorithms can understand, we can move on to the modeling proper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b34439",
   "metadata": {},
   "source": [
    "## Defining the **Functions**\n",
    "\n",
    "To start with, let us first define the functions and the values needed to easily train the model. First, we will be creating a list that will hold the [`dictionaries`](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) of scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19de834",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53224c44",
   "metadata": {},
   "source": [
    "Next, we will also declare the path where our trained models will be saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab4c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = './saved_models/Trad_ML/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbaa94b",
   "metadata": {},
   "source": [
    "After this, to abstract the way we save our models, let us define a function that will: (1) create the path where our model will be saved, (2) the model's file name, and (3) save the model to the specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models (model, model_name, vectorizer_name):\n",
    "    curr_directory = main_directory + model_name + '/' + vectorizer_name + '/'\n",
    "    \n",
    "    model_filename = curr_directory + 'model' + '.pkl'\n",
    "    \n",
    "    with open(model_filename, 'wb') as file:\n",
    "        pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240675b3",
   "metadata": {},
   "source": [
    "Next, we will be creating a function that will call the functions for the metrics (i.e., [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html), [`f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), [`hamming_loss`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html), and [`precision_recall_fscore_support`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)) that are used for the scoring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57def7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores (y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true = y_true, y_pred = y_pred) * 100\n",
    "    f1_micro_average = f1_score(y_true = y_true, y_pred = y_pred, average = 'micro') * 100\n",
    "    f1_macro_average = f1_score(y_true = y_true, y_pred = y_pred, average = 'macro') * 100\n",
    "    hamming_loss_score = hamming_loss(y_true = y_true, y_pred = y_pred) * 100\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred, average = 'micro')\n",
    "    \n",
    "    return accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision * 100, recall * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed791862",
   "metadata": {},
   "source": [
    "To be able to view the scores in a readable format, we also created a function that would print the metric's name and the score, side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall):\n",
    "    print('Accuracy: ', accuracy, '%')\n",
    "    print('F1 Macro Average: ', f1_macro_average, '%')\n",
    "    print('F1 Micro Average: ', f1_micro_average, '%')\n",
    "    print('Hamming Loss: ', hamming_loss_score, '%')\n",
    "    print('Precision: ', precision, '%')\n",
    "    print('Recall: ', recall, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e916c3",
   "metadata": {},
   "source": [
    "As we would be training six models, we would also be abstracting the way that we train the model by creating a function. In this function: (1) the model will be trained, (2) the model will be used to predict on the train set, (3) the score on the train predictions will be computed and printed, (4) the model will be used to predict on the test set, and (5) the model will be saved to the created directory. In addition, this function returns the trained model and the test predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99288ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(base_model, X_train, y_train, X_test, y_test, model_name, vectorizer_name):\n",
    "    test_predictions = np.zeros((len(y_test), 1))   \n",
    "                                                       \n",
    "    model = base_model\n",
    "    model.fit(X_train, y_train)   \n",
    "    \n",
    "    train_predictions = model.predict(X_train)                      \n",
    "    accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_train, train_predictions)    \n",
    "    print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)\n",
    "\n",
    "    test_predictions = model.predict(X_test)       \n",
    "    \n",
    "    save_models(model, model_name, vectorizer_name)\n",
    "    \n",
    "    return model, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212aa4b4",
   "metadata": {},
   "source": [
    "Last, we would also be creating a function that would be tuning the model through the use of [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Additionally, like in the training fnuction, the tuned model will be used to predict on the train set, and the scores of the predictions will be computed and printed. \n",
    "\n",
    "Afterwards, the model will also be used to predict on the test set, and then, the model will be saved, before returning the tuned model and the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac1621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_train_model(model, hyperparameters,\n",
    "                         X_train, y_train, \n",
    "                         X_test, y_test, \n",
    "                         model_name, vectorizer_name,\n",
    "                         scoring='accuracy', cv = 5):\n",
    "    \n",
    "    print('Tuning', str(model) + '...')\n",
    "        \n",
    "    model_cv = GridSearchCV(model, hyperparameters, cv = cv, scoring = scoring, n_jobs = -1)\n",
    "    model_cv.fit(X_train, y_train)\n",
    "        \n",
    "    train_predictions = model_cv.predict(X_train)                              \n",
    "    accuracy = accuracy_score(train_predictions, y_train)           \n",
    "        \n",
    "    test_predictions = model_cv.predict(X_test)               \n",
    "    \n",
    "    save_models(model_cv.best_estimator_, model_name, vectorizer_name)\n",
    "    \n",
    "    return model_cv.best_estimator_, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70e430",
   "metadata": {},
   "source": [
    "## Declaration of **Hyperparameter Space**\n",
    "\n",
    "Before we move on to the actual training proper, we will be defining the hyperparameter space for each of the machine learning algorithms.\n",
    "\n",
    "Let us start with the hyperparameter space of the [`Logistic Regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) models. For this, we will be tuning the **C** (inverse of regularization strength) and the **max_iter** (the maximum numbers of iterations allowed for solvers to converge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63411683",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hp_space = [{\n",
    "    'C' : [0.01, 0.1, 1, 10],\n",
    "    'max_iter' : [50, 100, 300, 600, 900, 1100] \n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003bfb3",
   "metadata": {},
   "source": [
    "Next, for the [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html), we will be tuning the value for **alpha** (Additive (Laplace/Lidstone) smoothing parameter) and the **fit_prior** (if the model will learn the prior probabilities of the classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_hp_space = [{\n",
    "    'alpha' : [0.0001, 0.001, 0.1, 1, 10, 100, 1000],\n",
    "    'fit_prior' : [True, False]\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14647874",
   "metadata": {},
   "source": [
    "Last, for the [`Random Forest Classifiers`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), there will be three hyperparameters that we want to tune: (1) **n_estimators** (the number of trees), (2) **max_depth** (the maximum depth allowed for the tree), and (3) **max_leaf_nodes** (the maximum number of leaf nodes allowed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec0ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hp_space = [{\n",
    "    'n_estimators' : [50, 100, 150],\n",
    "    'max_depth' : [None, 50, 100, 150],\n",
    "    'max_leaf_nodes' : [None, 50, 100]\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30a140",
   "metadata": {},
   "source": [
    "## Logistic Regression (TF-IDF Vectorizer)\n",
    "For our first model, we will be training and tuning a [`Logistic Regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) using inputs created using [`TF-IDF Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c5cf8",
   "metadata": {},
   "source": [
    "### Model Training \n",
    "As a starting point, let us first define an instance of [`Logistic Regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). The **n_jobs = -1** just means that all processors can be used for its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1fa5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c90bca",
   "metadata": {},
   "source": [
    "Using this instance, we will now be training the model using the function we previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc9195",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_tfidf, lr_test_predictions_tfidf = train_model (log_reg, \n",
    "                                                        tfidf_train, y_train, \n",
    "                                                        tfidf_test, y_test, \n",
    "                                                        'logreg', 'tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647282df",
   "metadata": {},
   "source": [
    "To fully understand how our model fares with the test data, we will be plotting the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) of the test data using the [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, lr_test_predictions_tfidf)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13144a",
   "metadata": {},
   "source": [
    "From this, we can see that the model incorrectly 5.74% of the negative samples, and 7.22% of the positive samples. Now, let us compute the scores of the model on the test set using the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5797654",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_test, lr_test_predictions_tfidf)   \n",
    "print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d63ac",
   "metadata": {},
   "source": [
    "From the scores, we can see that it managed to get 93.52% on the accuracy, which is considered as the main metric as the dataset is balanced. This is already considered high as the highest accuracy (so far) was 95.47% on a RoBERTa model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc06ef",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "We can now start tuning the [`Logistic Regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model by creating an instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d814a53",
   "metadata": {},
   "source": [
    "With this instance, we can now tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tuned_model_tfidf, lr_tuned_test_predictions_tfidf = tune_and_train_model (log_reg, lr_hp_space, \n",
    "                                                                              tfidf_train, y_train, \n",
    "                                                                              tfidf_test, y_test,\n",
    "                                                                              'logreg_tuned', 'tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb1200",
   "metadata": {},
   "source": [
    "From this, we can see that the tuned model has an **inverse of regularization strength of 10, and a maximum iteration value of 600**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5acb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tuned_model_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fce41",
   "metadata": {},
   "source": [
    "Now, let us visualize the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) of the test data through the [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f041cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, lr_tuned_test_predictions_tfidf)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d7c5ee",
   "metadata": {},
   "source": [
    "In this, we can see that the number of incorrectly labeled samples by the model decreased. It managed to correctly identify 97 more negative samples and 83 more positive samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cfb9ad",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Now, let us evaluate the performance of the model on the test set by computing its scores on the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_test, lr_tuned_test_predictions_tfidf)    \n",
    "\n",
    "temp_scores = {\n",
    "    'Model' : 'Logistic Regression',\n",
    "    'Vectorizer' : 'TF-IDF Vectorizer',\n",
    "    'Accuracy' : accuracy,\n",
    "    'F1 Micro Average' : f1_micro_average,\n",
    "    'F1 Macro Average' : f1_macro_average,\n",
    "    'Hamming Loss' : hamming_loss_score, \n",
    "    'Precision' : precision,\n",
    "    'Recall' : recall\n",
    "}  \n",
    "\n",
    "scores_list.append(temp_scores)\n",
    "\n",
    "print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a3977",
   "metadata": {},
   "source": [
    "From these scores, we can see that the accuracy score of the [`Logistic Regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) trained on [`TF-IDF Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) has increased by 0.38%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8a21f",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "To determine which words our model uses more in predicting, let us determine the importance of each of the feature (word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c694b4-7e4e-4e2a-9277-343cc72881da",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(estimator=lr_tuned_model_tfidf, \n",
    "                  feature_names= list(tfidf_vectorizer.get_feature_names()),\n",
    "                  top=(50, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d137b8d",
   "metadata": {},
   "source": [
    "From this, we can see that the top five words that the model mostly consider when predicting a positive label (i.e., **Suicidal**) are the words **suicide, suicidal, mei, helpi, and myselfi**. On the other hand, words with **bruh, teenages, crush, ted, and rant** are given more weight when predicting **Non-Suicidal** (i.e., a negative label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66dc533",
   "metadata": {},
   "source": [
    "## Logistic Regression (Count Vectorizer)\n",
    "Now, let us move on with the training and tuning of a  [`Logistic Regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model on a document-term matrix generated by a [`Count Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca81a8f",
   "metadata": {},
   "source": [
    "### Model Training \n",
    "To start with the model training, we will need to define an instance of a [`Logistic Regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model like we have done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36831e74",
   "metadata": {},
   "source": [
    "Then, we can continue with the model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf87b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_count, lr_test_predictions_count = train_model (log_reg, \n",
    "                                                        count_train, y_train, \n",
    "                                                        count_test, y_test, \n",
    "                                                        'logreg', 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e52a4",
   "metadata": {},
   "source": [
    "Like we have previously done, a [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) will be created to visualize the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ae8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, lr_test_predictions_count)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e96ff0",
   "metadata": {},
   "source": [
    "From this display, we can see that the model was incorrect in identifying 4.72% of the negative labels, and 10.15% of the positive label. To see the effect of these incorrectly taggeed samples in the metrics, let us compute the score of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_test, lr_test_predictions_count)   \n",
    "print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875f635",
   "metadata": {},
   "source": [
    "In these scores, we can see that the model received 92.57% score in accuracy, which is still considered high (as it was able to reach 90%). However, the first  [`Logistic Regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model still received higher scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee10a3",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Now, we can continue to tuning the model. To begin with, an instance of [`Logistic Regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9ace2",
   "metadata": {},
   "source": [
    "As we have already defined an instance of the model that we will be using as the base model, we can move on to tuning this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcae26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tuned_model_count, lr_tuned_test_predictions_count = tune_and_train_model (log_reg, lr_hp_space, \n",
    "                                                                              count_train, y_train, \n",
    "                                                                              count_test, y_test,\n",
    "                                                                              'logreg_tuned', 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99704ec3",
   "metadata": {},
   "source": [
    "As a result of tuning process, the identified best values of the hyperparameters for this model was an **inverse of regularization strength of 1, and a maximum iteration value of 1100**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb55e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tuned_model_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aab60d",
   "metadata": {},
   "source": [
    "To determine if there are more false positives or more false negatives in the predictions, we will be utilizing the [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) to create a visualization for the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) of the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, lr_tuned_test_predictions_count)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43109dcc",
   "metadata": {},
   "source": [
    "In this resulting visualization, we can see that there are more incorrectly tagged as negative compared to the number of samples that are tagged as positive even though they are actually negative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca6a24",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Now, let us evaluate the performance of the model on the test set by computing the scores using the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6228b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_test, lr_tuned_test_predictions_count)    \n",
    "\n",
    "temp_scores = {\n",
    "    'Model' : 'Logistic Regression',\n",
    "    'Vectorizer' : 'Count Vectorizer',\n",
    "    'Accuracy' : accuracy,\n",
    "    'F1 Micro Average' : f1_micro_average,\n",
    "    'F1 Macro Average' : f1_macro_average,\n",
    "    'Hamming Loss' : hamming_loss_score,\n",
    "    'Precision' : precision,\n",
    "    'Recall' : recall\n",
    "}  \n",
    "\n",
    "scores_list.append(temp_scores)\n",
    "\n",
    "print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655bbcad",
   "metadata": {},
   "source": [
    "Comparing the tuned model with the base [`Logistic Regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model, we can see that the tuning process helped improve the model's accuracy by 0.49%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6983b9",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Like in the previous model, we can move on with determining which words have more weights in labeling the samples as positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c80a9-ad77-45b2-aba4-4f216a906ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(estimator=lr_tuned_model_count, \n",
    "                  feature_names= list(count_vectorizer.get_feature_names()),\n",
    "                 top=(50,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786349d",
   "metadata": {},
   "source": [
    "For the [`Logistic Regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model on a document-term matrix generated by a [`Count Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), the words that are important for labeling text as positive are the words: **(1) mei, (2) helpi, (3) anymorei, (4) lifei, (5) iti**. This is unlike the previous model that focused on the words suicide and suicidal.\n",
    "\n",
    "However, for the labeling of negative, the model focuses on the words **teenagers, bruh, determination, dumping, and ted**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979bfad",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes (TF-IDF Vectorizer)\n",
    "The third model that we will be training is a [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model that utilized a document-term matrix generated by a [`TF-IDF Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606c404",
   "metadata": {},
   "source": [
    "### Model Training \n",
    "As our first step, we will first need to define a [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_nb = MultinomialNB ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a1b0a7",
   "metadata": {},
   "source": [
    "We pass the [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) object that we have instantiated to the function we have created to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tfidf, mnb_test_predictions_tfidf = train_model (multinomial_nb, \n",
    "                                                     tfidf_train, y_train, \n",
    "                                                     tfidf_test, y_test, \n",
    "                                                     'mnb', 'tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cadff8",
   "metadata": {},
   "source": [
    "To determine if there are more false positives or false negatives in our predictions, we will be generating the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) of the predictions, and then using [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, mnb_test_predictions_tfidf)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836fc6e",
   "metadata": {},
   "source": [
    "From the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html), we can see the this model recorded the lowest number of false negatives. However, it also recorded the highest number of false positives.\n",
    "\n",
    "We will also be using the test predictions to put a number on the performance of the model by computing its scores on the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3cca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_test, mnb_test_predictions_tfidf)   \n",
    "print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f9b68",
   "metadata": {},
   "source": [
    "Using these results, we can see that it managed to score 87.95% as its accuracy. This is lower than the accuracy scores that previous models received. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c42f74",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Like in the previous models, to start the tuning of the model, we will also need to create an instance of the [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48582354",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_nb = MultinomialNB ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a00c70",
   "metadata": {},
   "source": [
    "Afterwards, we will also pass this instance to the training and tuning function that we have previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tuned_model_tfidf, mnb_tuned_test_predictions_tfidf = tune_and_train_model (multinomial_nb, mnb_hp_space, \n",
    "                                                                                tfidf_train, y_train, \n",
    "                                                                                tfidf_test, y_test,\n",
    "                                                                                'mnb_tuned', 'tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63396b2c",
   "metadata": {},
   "source": [
    "From the tuning, we received a tuned model with the hyperparameter values: (1) **0.1 as its alpha**, and (2) **True as its fit_prior** (the default value for this hyperparmeter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba36005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tuned_model_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b035d2",
   "metadata": {},
   "source": [
    "Using the predictions of the tuned model, we will be using the [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) to show the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63957275",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, mnb_tuned_test_predictions_tfidf)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f30a0",
   "metadata": {},
   "source": [
    "From the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) of the tuned [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model, we can see that the number of false negatives increased while the number of false positives decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db633e9",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We can now move on to evaluating the performance of the model on the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_test, mnb_tuned_test_predictions_tfidf)    \n",
    "\n",
    "temp_scores = {\n",
    "    'Model' : 'Multinomial Naive Bayes',\n",
    "    'Vectorizer' : 'TF-IDF Vectorizer',\n",
    "    'Accuracy' : accuracy,\n",
    "    'F1 Micro Average' : f1_micro_average,\n",
    "    'F1 Macro Average' : f1_macro_average,\n",
    "    'Hamming Loss' : hamming_loss_score,\n",
    "    'Precision' : precision,\n",
    "    'Recall' : recall\n",
    "}  \n",
    "\n",
    "scores_list.append(temp_scores)\n",
    "\n",
    "print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e8687",
   "metadata": {},
   "source": [
    "From the results of predicting on the test set using the tuned model, it can be seen that the performance on the accuracy metric of the [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model improved by 2.74%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92832a95",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Now, let us also see which words were important for the model in classifying the instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ea2f6-ab5a-4c7b-bc07-017e0a3cecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tfidf_vectorizer.get_feature_names()\n",
    "zipped_neg = list(zip(words, mnb_tuned_model_tfidf.feature_log_prob_[0]))\n",
    "zipped_pos = list(zip(words, mnb_tuned_model_tfidf.feature_log_prob_[1]))\n",
    "sorted_zip_neg = sorted(zipped_neg, key=lambda t: t[1], reverse=True)\n",
    "sorted_zip_pos = sorted(zipped_pos, key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print(\"Positive Class\")  \n",
    "for each in sorted_zip_pos[:20]:\n",
    "    print(each)\n",
    "print(\"Negative Class\")    \n",
    "for each in sorted_zip_neg[:20]:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0bb731",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes (Count Vectorizer)\n",
    "Next, we will be using the document-term matrix produced by a [`Count Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to train a [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d253b30",
   "metadata": {},
   "source": [
    "### Model Training \n",
    "As done previously, we will start by creating an instance of [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dab77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_nb = MultinomialNB ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b7d9d",
   "metadata": {},
   "source": [
    "With this, we can proceed with training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_count, mnb_test_predictions_count = train_model (multinomial_nb, \n",
    "                                                     count_train, y_train, \n",
    "                                                     count_test, y_test, \n",
    "                                                     'mnb', 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6a356",
   "metadata": {},
   "source": [
    "To determine if there are more false positives or false negatives in our predictions, we will be generating the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) of the predictions, and then using [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ce3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, mnb_test_predictions_count)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ffbe73",
   "metadata": {},
   "source": [
    "Then, we can proceed with computing the score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d507bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_test, mnb_test_predictions_count)   \n",
    "print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de01ab",
   "metadata": {},
   "source": [
    "The [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model using the document-term matrix produced by  [`Count Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) received 89.78% as its accuracy. This is actually higher than the accuracy of the untuned [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model using the [`TF-IDF Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d43757",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "As the training is done, we can continue with the tuning of the model. As done previously, we will first generate an instance of the [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839af927",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_nb = MultinomialNB ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554ade9",
   "metadata": {},
   "source": [
    "With this object, we can now tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tuned_model_count, mnb_tuned_test_predictions_count = tune_and_train_model (multinomial_nb, mnb_hp_space, \n",
    "                                                                                count_train, y_train, \n",
    "                                                                                count_test, y_test,\n",
    "                                                                                'mnb_tuned', 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe45a6",
   "metadata": {},
   "source": [
    "After the tuning, we can see that the hyperparameter value that received the highest score for the model was the **alpha value of 0.1 and the fit_prior value of True**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813bc371",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tuned_model_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be3be9",
   "metadata": {},
   "source": [
    "Now, let us visualize the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) of the test data through the [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, mnb_tuned_test_predictions_count)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc8daba",
   "metadata": {},
   "source": [
    "From this, we can see that the model incorrectly 15.05% of the negative samples, and 3.91% of the positive samples. Now, let us compute the scores of the model on the test set using the metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43058b61",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Now, let us evaluate the performance of the model on the test set by computing its scores on the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_test, mnb_tuned_test_predictions_count)    \n",
    "\n",
    "temp_scores = {\n",
    "    'Model' : 'Multinomial Naive Bayes',\n",
    "    'Vectorizer' : 'Count Vectorizer',\n",
    "    'Accuracy' : accuracy,\n",
    "    'F1 Micro Average' : f1_micro_average,\n",
    "    'F1 Macro Average' : f1_macro_average,\n",
    "    'Hamming Loss' : hamming_loss_score,\n",
    "    'Precision' : precision,\n",
    "    'Recall' : recall\n",
    "}  \n",
    "\n",
    "scores_list.append(temp_scores)\n",
    "\n",
    "print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae9380",
   "metadata": {},
   "source": [
    "From the results of predicting on the test set using the tuned model, it can be seen that the performance on the accuracy metric of the [`Multinomial Naive Bayes`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model improved by 0.72%. Although, after tuning, the [`TF-IDF Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) now has higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f8711",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "To determine which words our model uses more in predicting, let us determine the importance of each of the feature (word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619e917-2fa2-4ca5-ba58-d0440b64ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = count_vectorizer.get_feature_names()\n",
    "zipped_neg = list(zip(words, mnb_tuned_model_count.feature_log_prob_[0]))\n",
    "zipped_pos = list(zip(words, mnb_tuned_model_count.feature_log_prob_[1]))\n",
    "sorted_zip_neg = sorted(zipped_neg, key=lambda t: t[1], reverse=True)\n",
    "sorted_zip_pos = sorted(zipped_pos, key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print(\"Positive Class\")  \n",
    "for each in sorted_zip_pos[:20]:\n",
    "    print(each)\n",
    "print(\"Negative Class\")    \n",
    "for each in sorted_zip_neg[:20]:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15025c92",
   "metadata": {},
   "source": [
    "## Random Forest Classifier (TF-IDF Vectorizer)\n",
    "Now, let us move on with the training and tuning of a [`Random Forest Classifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model on a document-term matrix generated by a [`TF-IDF Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).## Random Forest Classifier (TF-IDF Vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc87c0",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "To start with the model training, we will need to define an instance of a [`Random Forest Classifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model like we have done previously. Note that the **n_jobs = -1** just means that all processors can be used for its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1568e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba984b23",
   "metadata": {},
   "source": [
    "Using this instance, we will now be training the model using the function we previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tfidf, rf_test_predictions_tfidf = train_model (rf_classifier,\n",
    "                                                   tfidf_train, y_train, \n",
    "                                                   tfidf_test, y_test, \n",
    "                                                   'rf', 'tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb89328",
   "metadata": {},
   "source": [
    "To fully understand how our model fares with the test data, we will be plotting the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) of the test data using the [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35687d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, rf_test_predictions_tfidf)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a6f5f",
   "metadata": {},
   "source": [
    "From this, we can see that the model incorrectly 10.81% of the negative samples, and 11.67% of the positive samples. Now, let us compute the scores of the model on the test set using the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf19009",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_test, rf_test_predictions_tfidf)   \n",
    "print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06b8d0",
   "metadata": {},
   "source": [
    "From the scores, we can see that it managed to get 88.75% on the accuracy, which is lower than the score it received from the train data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907adc08",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Now, we can continue to tuning the model. To begin with, an instance of [`Random Forest Classifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e166b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982865f5",
   "metadata": {},
   "source": [
    "As we have already defined an instance of the model that we will be using as the base model, we can move on to tuning this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned_model_tfidf, rf_tuned_test_predictions_tfidf = tune_and_train_model (rf_classifier, rf_hp_space, \n",
    "                                                                              tfidf_train, y_train,\n",
    "                                                                              tfidf_test, y_test,\n",
    "                                                                              'rf_tuned', 'tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd39e3e",
   "metadata": {},
   "source": [
    "As a result of tuning process, the identified best values of the hyperparameters for this model was **150 for the number of trees in the forest**, with **no maximum number of value for its leaf nodes and depth**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540df367",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned_model_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc31648",
   "metadata": {},
   "source": [
    "To determine if there are more false positives or more false negatives in the predictions, we will be utilizing the [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) to create a visualization for the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) of the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f8d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, rf_tuned_test_predictions_tfidf)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceff5c9",
   "metadata": {},
   "source": [
    "In this resulting visualization, we can see that the number of false positive and the number of false negatives are actually near in value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d8c55",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Now, let us evaluate the performance of the model on the test set by computing the scores using the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef59c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_test, rf_tuned_test_predictions_tfidf)    \n",
    "\n",
    "temp_scores = {\n",
    "    'Model' : 'Random Forest Classifier',\n",
    "    'Vectorizer' : 'TF-IDF Vectorizer',\n",
    "    'Accuracy' : accuracy,\n",
    "    'F1 Micro Average' : f1_micro_average,\n",
    "    'F1 Macro Average' : f1_macro_average,\n",
    "    'Hamming Loss' : hamming_loss_score,\n",
    "    'Precision' : precision,\n",
    "    'Recall' : recall\n",
    "}  \n",
    "\n",
    "scores_list.append(temp_scores)\n",
    "\n",
    "print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954707c",
   "metadata": {},
   "source": [
    "Comparing the tuned model with the base [`Random Forest Classifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model, we can see that the tuning process helped improve the model's accuracy by 0.16%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b8945",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Like in the previous model, we can move on with determining which words have more weights in labeling the samples as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d603e8d7-f5cd-41ea-bb76-e9360270d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "rf_feature_importance = pd.DataFrame (data = {\n",
    "  'Features': features,\n",
    "  'Importance': np.round (model.feature_importances_, 4)\n",
    "})\n",
    "\n",
    "rf_feature_importance = rf_feature_importance.sort_values (by = 'Importance', ascending = False) \n",
    "rf_feature_importance.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7bb29",
   "metadata": {},
   "source": [
    "## Random Forest Classifier (Count Vectorizer)\n",
    "The last model that we will be training is a [`Random Forest Classifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model that was trained on a document-term matrix generated by a [`Count Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416aa19",
   "metadata": {},
   "source": [
    "### Model Training \n",
    "As done previously, we will start by creating an instance of [`Random Forest Classifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c790c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8800b278",
   "metadata": {},
   "source": [
    "With this, we can proceed with training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_count, rf_test_predictions_count = train_model (rf_classifier,\n",
    "                                                   count_train, y_train, \n",
    "                                                   count_test, y_test, \n",
    "                                                   'rf', 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68ed52",
   "metadata": {},
   "source": [
    "To fully understand how our model fares with the test data, we will be plotting the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) of the test data using the [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135fbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, rf_test_predictions_count)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514a018",
   "metadata": {},
   "source": [
    "From this, we can see that the model incorrectly 12.57% of the negative samples, and 11.38% of the positive samples. Now, let us compute the scores of the model on the test set using the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_test, rf_test_predictions_count)   \n",
    "print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb51fbe4",
   "metadata": {},
   "source": [
    "In this, we can see that the untuned [`Random Forest Classifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model received 88.02% as its accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867c94d",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "We can now start tuning the [`Random Forest Classifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model by creating an instance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c512e70",
   "metadata": {},
   "source": [
    "With this instance, we can now tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190fcc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned_model_count, rf_tuned_test_predictions_count = tune_and_train_model (rf_classifier, rf_hp_space, \n",
    "                                                                              count_train, y_train,\n",
    "                                                                              count_test, y_test,\n",
    "                                                                              'rf_tuned', 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc11b8",
   "metadata": {},
   "source": [
    "From the tuning, we can see that the hyperparameter values received by the model trained on a [`Count Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) document-term matrix is the same as the model trained on a [`TF-IDF Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) document-term matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87336f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned_model_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80604b",
   "metadata": {},
   "source": [
    "Now, let us visualize the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) of the test data through the [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, rf_tuned_test_predictions_count)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a14f23",
   "metadata": {},
   "source": [
    "In this, we can see that the model managed to correctly identify one (1) less negative samples and 21 more positive samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4796b6",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Now, let us evaluate the performance of the model on the test set by computing its scores on the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall = scores (y_test, rf_tuned_test_predictions_count)    \n",
    "\n",
    "temp_scores = {\n",
    "    'Model' : 'Random Forest Classifier',\n",
    "    'Vectorizer' : 'Count Vectorizer',\n",
    "    'Accuracy' : accuracy,\n",
    "    'F1 Micro Average' : f1_micro_average,\n",
    "    'F1 Macro Average' : f1_macro_average,\n",
    "    'Hamming Loss' : hamming_loss_score,\n",
    "    'Precision' : precision,\n",
    "    'Recall' : recall\n",
    "}  \n",
    "\n",
    "scores_list.append(temp_scores)\n",
    "\n",
    "print_scores (accuracy, f1_micro_average, f1_macro_average, hamming_loss_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868136dc",
   "metadata": {},
   "source": [
    "From these scores, we can see that the accuracy score of the [`Random Forest Classifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model that was trained on a document-term matrix generated by a [`Count Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) has increased by 0.04%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b5a227",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "To determine which words our model uses more in predicting, let us determine the importance of each of the feature (word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce85cf-ff50-47a4-828f-7fe6f31f60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = count_vectorizer.get_feature_names()\n",
    "\n",
    "rf_feature_importance = pd.DataFrame (data = {\n",
    "  'Features': features,\n",
    "  'Importance': np.round (model.feature_importances_, 4)\n",
    "})\n",
    "\n",
    "rf_feature_importance = rf_feature_importance.sort_values (by = 'Importance', ascending = False) \n",
    "rf_feature_importance.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c6680",
   "metadata": {},
   "source": [
    "# **Model Scores Summary**\n",
    "\n",
    "As a summary, we can see the the model that received the best  score for all of the metrics is the tuned [`Logistic Regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model, which utilized [`TF-IDF Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) as its feature engineering. Meanwhile, the model with the worst scores is the [`Random Forest Classifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) with [`Count Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) as its feature engineering. \n",
    "\n",
    "\n",
    "Also, it is important to note that for all of the models, the [`TF-IDF Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) performed better than the [`Count Vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c97b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores_list).sort_values(['Accuracy', 'F1 Micro Average', 'F1 Macro Average', 'Hamming Loss', 'Precision', 'Recall'], ascending = False).reset_index(drop = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
