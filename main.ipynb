{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3058ede7",
   "metadata": {},
   "source": [
    "# Insert Title Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb937d43",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc693a71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53dbd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import download, classify, corpus\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4cf4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d186e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec929cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f518d",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898287c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fakenews1 = pd.read_csv('data/fake news dataset.csv')\n",
    "df_fakenews1.rename(columns={'article': 'Content'}, inplace=True) #renamed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53113707",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fakenews2 = pd.read_csv('data/fake_or_real_news.csv')\n",
    "df_fakenews2.rename(columns={'text': 'Content'}, inplace=True) #renamed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining two dataset into a single DataFrame\n",
    "df_FakeNews = pd.concat([df_fakenews1, df_fakenews2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68fa9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning new values (raplacing the existing values 'REAL' and 'FAKE' with 0 and 1, respectively in the 'label' column)\n",
    "df_FakeNews.loc[:, 'label'] = df_FakeNews['label'].replace({'REAL': 0, 'FAKE': 1})\n",
    "df_FakeNews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a2c9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39585e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading news sites datasets and storing into DataFrames\n",
    "df_rappler = pd.read_csv('data/rap_dataframe.csv')\n",
    "df_rappler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gma1 = pd.read_csv('data/gma-10000.csv')\n",
    "df_gma2 = pd.read_csv('data/gma_dataframe.csv')\n",
    "\n",
    "df_GMA = pd.concat([df_gma1,df_gma2], ignore_index=True)\n",
    "df_GMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_FakeNews = df_FakeNews.drop_duplicates()\n",
    "df_FakeNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rappler = df_rappler.drop_duplicates()\n",
    "df_rappler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a63464",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GMA = df_GMA.drop_duplicates()\n",
    "df_GMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"df_FakeNews\",df_FakeNews.head(),\n",
    "        \"df_rappler\", df_rappler.head(), \n",
    "        \"df_GMA\", df_GMA.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.concat([df_FakeNews,df_rappler, df_GMA], ignore_index=True)\n",
    "df_news['label'] = pd.to_numeric(df_news['label'], errors='coerce').fillna(0.0)\n",
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_news = df_news.drop_duplicates()\n",
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['label'] = df_news['label'].astype(int)\n",
    "df_news = df_news.drop([\"Unnamed: 0\", \"Link\", \"Author\", \"title\"], axis=1)\n",
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabba766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newline_tab(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').replace('\\r\\n\\r', ' ')\n",
    "    else:\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38044a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_backslashes(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.replace('\\\\', '')\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa36e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to 'Content' column in df_News\n",
    "df_news['Content'] = df_news['Content'].apply(remove_newline_tab)\n",
    "print(df_news.loc[0, 'Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to remove links from text\n",
    "def remove_links(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    \n",
    "    # Replace URLs with an empty string\n",
    "    return re.sub(url_pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ffc4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to 'Content' column in df_news\n",
    "df_news['text'] = df_news['Content'].astype(str).apply(remove_links)\n",
    "df_news = df_news.drop ('Content', axis = 1)\n",
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_images(text):\n",
    "    # Define a regular expression pattern to match base64-encoded strings (images)\n",
    "    base64_pattern = r\"data:image\\/(png|jpg|jpeg|gif|bmp);base64,[A-Za-z0-9+/=]+\"\n",
    "\n",
    "    # Use the re.sub() function to replace the base64-encoded strings with an empty string\n",
    "    cleaned_text = re.sub(base64_pattern, '', text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['text'] = df_news['text'].astype(str).apply(remove_images)\n",
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f9bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news [df_news.duplicated ()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa81fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news.drop_duplicates ()\n",
    "df_news = df_news.reset_index (drop = True)\n",
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f65e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d7d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect the language of a text using NLTK\n",
    "def detect_language(text):\n",
    "    words = wordpunct_tokenize(text.lower())\n",
    "    if len(words) == 0:\n",
    "        return 'fil'\n",
    "    english_word_count = sum(1 for word in words if word in english_words)\n",
    "    english_word_rate = english_word_count / len(words)\n",
    "    \n",
    "    threshold = 0.5\n",
    "    \n",
    "    return 'en' if english_word_rate >= threshold else 'fil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the detect_language function to 'Content' column to create a new column 'language'\n",
    "df_news['language'] = df_news['text'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe84ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate news articles into English and Filipino datasets\n",
    "df_english_news = df_news[df_news['language'] == 'en']\n",
    "df_filipino_news = df_news[df_news['language'] == 'fil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e3c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English News Dataset\n",
    "df_english_news = df_english_news.drop(columns='language')\n",
    "df_english_news = df_english_news.reset_index (drop = True)\n",
    "df_english_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b94b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filipino News Dataset\n",
    "df_filipino_news = df_filipino_news.drop(columns='language')\n",
    "df_filipino_news = df_filipino_news.reset_index (drop = True)\n",
    "df_filipino_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving of filipino dataset to CSV file\n",
    "df_filipino_news.to_csv('filipino_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21250be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving of english dataset to CSV file\n",
    "df_english_news.to_csv('english_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving of Cleaned Data to CSV file\n",
    "df_news.to_csv('cleaned_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d96ee8",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata = pd.read_csv('cleaned_data.csv')\n",
    "df_cleaneddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8cd3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986030e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cleaneddata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95053e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_cleaneddata.label\n",
    "print(f'Ratio of real and fake news:')\n",
    "y.value_counts(normalize=True).rename({0: 'real', 1: 'fake'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa25a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.isnull().sum().plot(kind=\"barh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152703bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata = df_cleaneddata.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b383618",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df_cleaneddata[df_cleaneddata[\"label\"] == 1][\"Content\"].str.len(), bins, alpha=0.5, label=\"Fake\", color=\"#FF5733\")\n",
    "plt.hist(df_cleaneddata[df_cleaneddata[\"label\"] == 0][\"Content\"].str.len(), bins, alpha=0.5, label=\"Real\", color=\"#33FFB8\")\n",
    "\n",
    "plt.title('Distribution of Text Length for Fake/Real News')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab404f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['fake', 'real'] \n",
    "label_count = df_cleaneddata.label.value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=label_count.index, y=label_count)\n",
    "plt.title('Distribution of Fake/Real News',fontsize =14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ' '.join(title for title in df_cleaneddata['Content'])\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white', \n",
    "    max_words=300,\n",
    "    width=800, \n",
    "    height=400,\n",
    ").generate(titles)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822eec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_cleaneddata['Content'] = df_cleaneddata['Content'].apply(lambda x: tokenizer.tokenize(x))\n",
    "print(df_cleaneddata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59bbd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df_cleaneddata['Content'] = df_cleaneddata['Content'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "print(df_cleaneddata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ac597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata['Content'] = df_cleaneddata['Content'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in df_cleaneddata['Content']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc82132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector_Tfidf(df, col):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=2000)\n",
    "    vectorizer.fit(df[col])\n",
    "    return vectorizer.transform(df_cleaneddata[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_tfidf = to_vector_Tfidf(df_cleaneddata, 'Content')\n",
    "print(\"Shape of the tfidf vector: \", text_vector_tfidf.shape)\n",
    "print(text_vector_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d175b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata = df_cleaneddata[['Content']].copy(deep=True)\n",
    "df_cleaneddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a15f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata['length'] = df_cleaneddata['Content'].str.count(' ') + 1\n",
    "df_cleaneddata['LoR'] = df_cleaneddata['Content'].str.len()\n",
    "df_cleaneddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf0c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata[\"length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401936fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata[\"LoR\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ee42c",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b69615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec63bc36",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26c4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "028fab13",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b57fa",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
