{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3058ede7",
   "metadata": {},
   "source": [
    "# Insert Title Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb937d43",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc693a71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08df81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f518d",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898287c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading fake news datasets and storing into DataFrames\n",
    "df_fakenews1 = pd.read_csv('data/fake news dataset.csv')\n",
    "df_fakenews1.rename(columns={'article': 'Content'}, inplace=True) #renamed column\n",
    "\n",
    "df_fakenews2 = pd.read_csv('data/fake_or_real_news.csv')\n",
    "df_fakenews2.rename(columns={'text': 'Content'}, inplace=True) #renamed column\n",
    "\n",
    "#combining two dataset into a single DataFrame\n",
    "df_FakeNews = pd.concat([df_fakenews1, df_fakenews2], ignore_index=True)\n",
    "\n",
    "#assigning new values (raplacing the existing values 'REAL' and 'FAKE' with 0 and 1, respectively in the 'label' column)\n",
    "df_FakeNews.loc[:, 'label'] = df_FakeNews['label'].replace({'REAL': 0, 'FAKE': 1})\n",
    "df_FakeNews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a2c9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39585e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading news sites datasets and storing into DataFrames\n",
    "df_Rappler = pd.read_csv('rap_dataframe.csv')\n",
    "df_Rappler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gma1 = pd.read_csv('data/gma-10000')\n",
    "\n",
    "df_gma2 = pd.read_csv('gma_dataframe')\n",
    "\n",
    "df_GMA = pd.concat([df_gma1,df_gma2], ignore_index=True)\n",
    "df_GMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_FakeNews = df_FakeNews.drop_duplicates()\n",
    "df_Rappler = df_Rappler.drop_duplicates()\n",
    "df_GMA = df_GMA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"df_FakeNews\",df_FakeNews.head(),\n",
    "        \"df_Rappler\", df_Rappler.head(), \n",
    "        \"df_GMA\", df_GMA.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_News = pd.concat([df_FakeNews,df_Rappler, df_GMA], ignore_index=True)\n",
    "df_News['label'] = pd.to_numeric(df_News['label'], errors='coerce').fillna(0.0)\n",
    "\n",
    "# Drop duplicates\n",
    "df_News = df_News.drop_duplicates()\n",
    "df_News\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabba766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to remove '\\n' and '\\t'\n",
    "def remove_newline_tab(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.replace(r'\\n', ' ').replace(r'\\t', ' ')\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply preprocessing to 'Content' column in df_News\n",
    "df_News['Content'] = df_News['Content'].apply(remove_newline_tab)\n",
    "df_News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to remove links from text\n",
    "def remove_links(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    \n",
    "    # Replace URLs with an empty string\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "    # Apply preprocessing to 'Content' column in df_News\n",
    "df_News['Content'] = df_News['Content'].astype(str).apply(remove_links)\n",
    "df_News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_images(text):\n",
    "    # Define a regular expression pattern to match base64-encoded strings (images)\n",
    "    base64_pattern = r\"data:image\\/(png|jpg|jpeg|gif|bmp);base64,[A-Za-z0-9+/=]+\"\n",
    "\n",
    "    # Use the re.sub() function to replace the base64-encoded strings with an empty string\n",
    "    cleaned_text = re.sub(base64_pattern, '', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "df_News['Content'] = df_News['Content'].astype(str).apply(remove_images)\n",
    "df_News\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79850218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving of Cleaned Data to CSV file\n",
    "df_News.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d96ee8",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata = pd.read_csv('cleaned_data.csv')\n",
    "df_cleaneddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8cd3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986030e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing top 5 authors\n",
    "d = df_cleaneddata['Author'].value_counts().sort_values(ascending=False).head(5)\n",
    "d = pd.DataFrame(d)\n",
    "d = d.reset_index()\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(15,4))\n",
    "sns.barplot(x='index', y='Author', data=d)\n",
    "plt.xlabel(\"\\nAuthors\")\n",
    "plt.ylabel(\"Number of Articles written\")\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_cleaneddata.label\n",
    "print(f'Ratio of real and fake news:')\n",
    "y.value_counts(normalize=True).rename({1: 'real', 0: 'fake'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.drop([\"Unnamed: 0\", \"Link\", \"Author\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa25a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.isnull().sum().plot(kind=\"barh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152703bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc57f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b383618",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd19642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata[\"title_Content\"] = df_cleaneddata[\"title\"] + df_cleaneddata[\"Content\"]\n",
    "df_cleaneddata[\"body_len\"] = df_cleaneddata[\"title_Content\"].apply(lambda x: len(x) - x.count(\" \"))\n",
    "df_cleaneddata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df_cleaneddata[df_cleaneddata[\"label\"]== 1][\"body_len\"], bins, alpha=0.5, label=\"Fake\", color=\"#FF5733\")\n",
    "plt.hist(df_cleaneddata[df_cleaneddata[\"label\"]== 0][\"body_len\"], bins, alpha=0.5, label=\"Real\", color=\"#33FFB8\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab404f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['fake', 'real'] \n",
    "label_count = df_cleaneddata.label.value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=label_count.index, y=label_count)\n",
    "plt.title('Distribution of Fake/Real News',fontsize =14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ' '.join(title for title in df_cleaneddata['title'])\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white', \n",
    "    max_words=300,\n",
    "    width=800, \n",
    "    height=400,\n",
    ").generate(titles)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822eec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_cleaneddata['Content'] = df_cleaneddata['Content'].apply(lambda x: tokenizer.tokenize(x))\n",
    "print(df_cleaneddata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59bbd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df_cleaneddata['Content'] = df_cleaneddata['Content'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "print(df_cleaneddata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ac597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata['Content'] = df_cleaneddata['Content'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in df_cleaneddata['Content']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc82132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector_Tfidf(df, col):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=2000)\n",
    "    vectorizer.fit(df[col])\n",
    "    return vectorizer.transform(df_cleaneddata[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_tfidf = to_vector_Tfidf(df_cleaneddata, 'Content')\n",
    "print(\"Shape of the tfidf vector: \", text_vector_tfidf.shape)\n",
    "print(text_vector_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d175b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata = df_cleaneddata[['Content']].copy(deep=True)\n",
    "df_cleaneddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a15f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata['length'] = df_cleaneddata['Content'].str.count(' ') + 1\n",
    "df_cleaneddata['LoR'] = df_cleaneddata['Content'].str.len()\n",
    "df_cleaneddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf0c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata[\"length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401936fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata[\"LoR\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ee42c",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b69615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec63bc36",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26c4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "028fab13",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b57fa",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
