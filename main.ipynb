{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3058ede7",
   "metadata": {},
   "source": [
    "# Insert Title Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb937d43",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc693a71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import download, classify, corpus\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f518d",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898287c",
   "metadata": {},
   "source": [
    "The two Fake News Datasets is loaded and stored into separate DataFrames, df_fakenews1, and df_fakenews2. After reading the datasets, the 'article' column in df_fakenews1 to 'Content'. Similarly, in df_fakenews2, the 'text' column is renamed to 'Content'. Then the two separate DataFrames is merged together, df_fakenews1 and df_fakenews2, into a single DataFrame called df_FakeNews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fakenews1 = pd.read_csv('data/fake news dataset.csv')\n",
    "df_fakenews1.rename(columns={'article': 'Content'}, inplace=True) #renamed column\n",
    "\n",
    "\n",
    "df_fakenews2 = pd.read_csv('/data/fake_or_real_news.csv')\n",
    "df_fakenews2.rename(columns={'text': 'Content'}, inplace=True) #renamed column\n",
    "\n",
    "#combining two dataset into a single DataFrame\n",
    "df_FakeNews = pd.concat([df_fakenews1, df_fakenews2], ignore_index=True)\n",
    "\n",
    "#assigning new values (raplacing the existing values 'REAL' and 'FAKE' with 0 and 1, respectively in the 'label' column)\n",
    "df_FakeNews['label'] = df_FakeNews['label'].replace({'REAL': 0, 'FAKE': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e825b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FakeNews['label'] = df_FakeNews['label'].replace({0: 1, 1: 0})\n",
    "df_FakeNews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a2c9a",
   "metadata": {},
   "source": [
    "The News Sites Datasets is loaded and stored into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39585e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rappler = pd.read_csv('data/rap_dataframe.csv')\n",
    "df_rappler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gma1 = pd.read_csv('data/gma-10000.csv')\n",
    "\n",
    "df_gma2 = pd.read_csv('data/gma_dataframe.csv')\n",
    "\n",
    "df_GMA = pd.concat([df_gma1,df_gma2], ignore_index=True)\n",
    "df_GMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_FakeNews = df_FakeNews.drop_duplicates()\n",
    "df_rappler = df_rappler.drop_duplicates()\n",
    "df_GMA = df_GMA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"df_FakeNews\",df_FakeNews.head(),\n",
    "        \"df_rappler\", df_rappler.head(), \n",
    "        \"df_GMA\", df_GMA.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89be304",
   "metadata": {},
   "source": [
    "All Datasets is merged together into one DataFrame. After that, any duplicate rows is removed from df_News."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.concat([df_FakeNews,df_rappler, df_GMA], ignore_index=True)\n",
    "df_news['label'] = pd.to_numeric(df_news['label'], errors='coerce').fillna(0.0)\n",
    "\n",
    "# Drop duplicates\n",
    "df_news = df_news.drop_duplicates()\n",
    "df_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8261c6",
   "metadata": {},
   "source": [
    "Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d41de0",
   "metadata": {},
   "source": [
    "We dropped all unecessary columns like 'Unnamed: 0', 'Link', 'Author', 'title'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['label'] = df_news['label'].astype(int)\n",
    "df_news.drop([\"Unnamed: 0\", \"Link\", \"Author\", \"title\"], axis=1, inplace=True)\n",
    "df_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acadf1c",
   "metadata": {},
   "source": [
    " Data often contains unwanted characters or formatting that can make it challenging to work with. So, to remove specific characters from a text, we use remove_newline_tab and remove_backslashes to remove  \\n, \\t, \\r, and \\r\\n\\r characters, as well as remove backlash characters from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabba766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newline_tab(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').replace('\\r\\n\\r', ' ')\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_backslashes(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.replace('\\\\', '')\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply preprocessing to 'Content' column in df_News\n",
    "df_News['Content'] = df_News['Content'].apply(remove_newline_tab)\n",
    "print(df_News.loc[0, 'Content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712abd9",
   "metadata": {},
   "source": [
    "We also remove any web links or URLs that might be present in the text of the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to remove links from text\n",
    "def remove_links(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    \n",
    "    # Replace URLs with an empty string\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "    # Apply preprocessing to 'Content' column in df_news\n",
    "df_news['text'] = df_news['text'].astype(str).apply(remove_links)\n",
    "df_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b0e32",
   "metadata": {},
   "source": [
    "Any images that might be embedded in the text is also removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_images(text):\n",
    "    # Define a regular expression pattern to match base64-encoded strings (images)\n",
    "    base64_pattern = r\"data:image\\/(png|jpg|jpeg|gif|bmp);base64,[A-Za-z0-9+/=]+\"\n",
    "\n",
    "    # Use the re.sub() function to replace the base64-encoded strings with an empty string\n",
    "    cleaned_text = re.sub(base64_pattern, '', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "df_news['text'] = df_news['text'].astype(str).apply(remove_images)\n",
    "df_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917da05",
   "metadata": {},
   "source": [
    "After cleaning the data, it is then saved into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79850218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving of Cleaned Data to CSV file\n",
    "df_news.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8a08e",
   "metadata": {},
   "source": [
    "The data contains articles that is in the English language and Filipino language. With that, we sort these articles into two datasets: one for English articles and another for Filipino articles. We use Natural Language Toolkit library for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931428fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English words corpus from NLTK\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Function to detect the language of a text using NLTK\n",
    "def detect_language(text):\n",
    "    words = wordpunct_tokenize(text.lower())\n",
    "    if len(words) == 0:\n",
    "        return 'fil'\n",
    "    english_word_count = sum(1 for word in words if word in english_words)\n",
    "    english_word_rate = english_word_count / len(words)\n",
    "    \n",
    "    threshold = 0.75\n",
    "    \n",
    "    return 'en' if english_word_rate >= threshold else 'fil'\n",
    "\n",
    "# Apply the detect_language function to 'Content' column to create a new column 'language'\n",
    "df_News['language'] = df_News['Content'].apply(detect_language)\n",
    "\n",
    "# Separate news articles into English and Filipino datasets\n",
    "df_english_news = df_News[df_News['language'] == 'en']\n",
    "df_filipino_news = df_News[df_News['language'] == 'fil']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383cc34",
   "metadata": {},
   "source": [
    "The language column is dropped from the English and Filipino News Datasets as it would not be needed. After that, like the cleaned dataset, it is also saved into CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efaa030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English News Dataset\n",
    "df_english_news = df_News[df_News['language'] == 'en'].copy()\n",
    "df_english_news.drop(columns='language', inplace=True)\n",
    "df_english_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ad9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving of english news to CSV file\n",
    "df_english_news.to_csv('english_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88fc6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filipino News Dataset\n",
    "df_filipino_news = df_News[df_News['language'] == 'fil'].copy()\n",
    "df_filipino_news.drop(columns='language', inplace=True)\n",
    "df_filipino_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving of Filipino news to CSV file\n",
    "df_filipino_news.to_csv('/filipino_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe84ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d96ee8",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata = pd.read_csv('cleaned_data.csv')\n",
    "df_cleaneddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8cd3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc1dd9",
   "metadata": {},
   "source": [
    "The DataFrame contains 24,041 rows and 2 columns. There are 24,041 non-null entries in the label column. However, there are 2 missing values (non-null count is 24,039 instead of 24,041) in the Content  column, meaning there are no missing values (null values) in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986030e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cfdefb",
   "metadata": {},
   "source": [
    "Upon analyzing the \"label\" column of the DataFrame \"df_cleaneddata,\" which contains information about real and fake news, we can determine the ratio of real and fake news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_cleaneddata ['label']\n",
    "print(f'Ratio of real and fake news:')\n",
    "y.value_counts(normalize=True).rename({1: 'real', 0: 'fake'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa25a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.isnull().sum().plot(kind=\"barh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152703bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b383618",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd093cce",
   "metadata": {},
   "source": [
    "We create a histogram to compare the typical lengths of fake and real news articles and identify any potential differences between the two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df_cleaneddata[df_cleaneddata[\"label\"] == 1][\"text\"].str.len(), bins, alpha=0.5, label=\"Fake\", color=\"#FF5733\")\n",
    "plt.hist(df_cleaneddata[df_cleaneddata[\"label\"] == 0][\"text\"].str.len(), bins, alpha=0.5, label=\"Real\", color=\"#33FFB8\")\n",
    "\n",
    "plt.title('Distribution of Text Length for Fake/Real News')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee829c",
   "metadata": {},
   "source": [
    "Apart from that, we also visualize the distribution of fake news and real news through a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab404f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['fake', 'real'] \n",
    "label_count = df_cleaneddata.label.value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=label_count.index, y=label_count)\n",
    "plt.title('Distribution of Fake/Real News',fontsize =14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687d843d",
   "metadata": {},
   "source": [
    "Two new columns is added, the length column which represent the word count for each entry in the Content column, and Length of Content column which represents the length of each content entry in terms of the number of characters. This is to provide additional information about the content's length (in words and character) for each entry in the DataFrame which could be usedul for futher analysis to better understant the characteristics of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c0f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata['length'] = df_cleaneddata['Content'].str.count(' ') + 1\n",
    "df_cleaneddata['LoR'] = df_cleaneddata['Content'].str.len()\n",
    "df_cleaneddata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb832eb",
   "metadata": {},
   "source": [
    "df_cleaneddata[\"length\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851dad1",
   "metadata": {},
   "source": [
    "df_cleaneddata[\"LoR\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99391814",
   "metadata": {},
   "source": [
    "We also visualize the words from the articles where the size of each word corresponds to its frequency in the text. The larger the word, the more frequent it appears in the wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ' '.join(title for title in df_cleaneddata['text'])\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white', \n",
    "    max_words=300,\n",
    "    width=800, \n",
    "    height=400,\n",
    ").generate(titles)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822eec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_cleaneddata['text'] = df_cleaneddata['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "print(df_cleaneddata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59bbd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df_cleaneddata['text'] = df_cleaneddata['text'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "print(df_cleaneddata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ac597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaneddata['text'] = df_cleaneddata['text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in df_cleaneddata['text']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc82132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate fake and real news DataFrames\n",
    "df_fake_news = df_cleaneddata[df_cleaneddata['label'] == 1]\n",
    "df_real_news = df_cleaneddata[df_cleaneddata['label'] == 0]\n",
    "\n",
    "# Combine texts for word clouds\n",
    "fake_news_text = ' '.join(text for text in df_fake_news['Content'])\n",
    "real_news_text = ' '.join(text for text in df_real_news['Content'])\n",
    "\n",
    "# Generate word cloud for fake news\n",
    "wordcloud_fake = WordCloud(width=800, height=400, background_color='white', max_words=300).generate(fake_news_text)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud_fake, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Fake News')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Generate word cloud for real news\n",
    "wordcloud_real = WordCloud(width=800, height=400, background_color='white', max_words=300).generate(real_news_text)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud_real, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Real News')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ee42c",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b69615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec63bc36",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26c4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "028fab13",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b57fa",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
