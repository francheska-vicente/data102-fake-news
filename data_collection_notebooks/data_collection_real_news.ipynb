{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection Part 1**: Web Scraping <u>Real News</u>\n",
    "\n",
    "This notebook demonstrates the first part of our data collection which is web scraping real news from well-known and considered as reliable news sites in the Philippines. \n",
    "\n",
    "The **news sites involved** in collecting real news are the following: \n",
    "1. [`GMA News`](https://www.gmanetwork.com/news/topstories/)\n",
    "2. [`Rappler`](https://www.rappler.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-a45YrMKlE2"
   },
   "source": [
    "# Importing Libraries\n",
    "To start, we will be importing libraries that would help us perform web scraping and data processing properly.\n",
    "\n",
    "## Basic Libraries\n",
    "*  [`time`](https://docs.python.org/3/library/time.html): provides functions for handling time related tasks \n",
    "> this allows us to enable pauses for the loops that we will run later on\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/) : has functions for data analysis and manipulation\n",
    "> this allows us to obtain and organize all scraped data in a format that can be used for Exploratory Data Analysis (EDA) and for creating a model for fake news detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sP_GeHWhKrpV"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yz_1GVS2MHH8"
   },
   "source": [
    "## Web Scraping Library: `Selenium`\n",
    "> The Selenium library will be used for  web scraping GMA and Rappler since navigation through multiple pages will be performed on both sites.\n",
    "\n",
    "*    `from selenium import webdriver` enables automation of web browser\n",
    "*    `from selenium.webdriver.common.keys import Keys` enables simulation of keyboard keys\n",
    "*   `from selenium.webdriver.common.by import By` helps with locating elements in the webpage through XPath\n",
    "*   `from selenium.webdriver.chrome.options import Options` enables us to specify certain preferences when initializing the WebDriver, aids in preventing multiple windows from initializing when performing the loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWwceB84JjQl"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations for Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MepOlkUMMXdW"
   },
   "source": [
    "## Preparing [`Chrome WebDriver`](https://chromedriver.chromium.org/downloads) for Selenium Web Scraping\n",
    "\n",
    "Since the needed libraries have already been imported, we will now **set up our Chrome WebDriver** before we start web scraping using the Selenium library. \n",
    "\n",
    "* [`Chrome WebDriver`](https://chromedriver.chromium.org/getting-started) is a driver used for the Selenium WebDriver to have control on the Chrome window during web scraping.\n",
    "\n",
    "### 1. To start setting up the driver, download the drive from the [`ChromeDriver site`](https://chromedriver.chromium.org/downloads). \n",
    "It is recommended to put the downloaded file in a file location that you would be easy to locate it or in the same file location as the project. This is so that performing the next step would be easy for you.\n",
    "\n",
    "### 2. Set the driver path \n",
    "This part necessitates the user to specify their own driver path in their device and store it in a variable, in this case, the driver path will be stored in the `driver_path` variable. This will enable scraping through initializing the Chrome WebDriver in the later step of the set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-3AR-bmMcSn"
   },
   "outputs": [],
   "source": [
    "# `driver_path` : contains the path where the WebDriver is found\n",
    "driver_path=\"/Users/beatricebanzon/Desktop/dlsu/col/2/T3 22.23/DATA102/chromedriver_mac_arm64/chromedriver.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jnpth1-DOLW4"
   },
   "source": [
    "### 3. Specify Chrome Preferences\n",
    "In this step, we will  we will specify opting to disable opening a visible GUI each time the subsequent loops are ran. With the help of the [`Options()`](https://chromedriver.chromium.org/capabilities), we would be able to specify our chrome preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIDhJqAjnYrI"
   },
   "outputs": [],
   "source": [
    "# `chrome_options` : handles the  specified chrome preferences\n",
    "chrome_options = Options()\n",
    "\n",
    "#  \"--headless=new\" : disables opening new windows or any GUI even if the browser is running\n",
    "chrome_options.add_argument(\"--headless=new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Initialize Chrome Driver\n",
    "\n",
    "Aas for the last step for setting up the Chrome Driver, we will be needing the driver path and chrome preferences we specified from Steps 2 and 3. \n",
    "\n",
    "To recall, we have stored these values to the following variables:\n",
    "* `driver_path` : contains the file path where your Chrome WebDriver is found\n",
    "* `chrome_options` : contains the chrome preferences we specified from Step 3\n",
    "\n",
    "Through inserting these two variables into the [`webdriver.Chrome()`](https://sites.google.com/chromium.org/driver/capabilities?authuser=0), we will now initialize the driver to be used later for web scraping and store it to a variable named `driver`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `driver`: contains the function for setting the Chrome's behavior once the selenium starts web scraping\n",
    "driver = webdriver.Chrome(driver_path, chrome_options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLA9k_QHOYyU"
   },
   "source": [
    "## Setting the Delay Between Scraping Requests\n",
    "The `pause` variable holds the <u>**number of delay in seconds** between requests</u> during the whole process of web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebAWDw2knZQ6"
   },
   "outputs": [],
   "source": [
    "pause = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping News Sites\n",
    "\n",
    "This section is divided into two major parts which are: (1) Web Scraping GMA News and (2) Web Scraping Rappler. Both sections of web scraping will be performed with the help of Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhPQv06wMRrQ"
   },
   "source": [
    "## Web Scraping `GMA News`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIzYtfjZM7ce"
   },
   "source": [
    "### GMA News Pages\n",
    "> The pages containing the links of GMA's articles will be scraped.\n",
    "\n",
    "* gma_url : url of GMA containing the articles\n",
    "* counter : used to limit the number of scraped pages\n",
    "* gma_master : container for the links\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbUGYqzzthgv"
   },
   "source": [
    "The url below will be scraped - notably, it needs scrolling to load subsequent pages. In the said page, all the articles may be seen; thus, we start by getting the links of each article first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Izen0VCRM9Wc"
   },
   "outputs": [],
   "source": [
    "gma_url=f\"https://www.gmanetwork.com/news/archives/topstories/\"\n",
    "counter = 1\n",
    "gma_master = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcqcp34PPqIV"
   },
   "source": [
    "In the following code, XPath is used to locate and obtain the elements from the\n",
    "HTML page. The loop below will be ran as the site scrolls.\n",
    "\n",
    "* '.find_elements' : for locating multiple elements (in this case, articles)\n",
    "* '.get_attribute' : for retrieving a specific attribute inside the element (in this case, for getting the link found in each element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Px62B5tZnWbK"
   },
   "outputs": [],
   "source": [
    "driver.get(gma_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1s4PNLCNBGs"
   },
   "outputs": [],
   "source": [
    "checker = True\n",
    "while checker:\n",
    "    url = driver.current_url\n",
    "    print (gma_url)\n",
    "\n",
    "    time.sleep(pause)\n",
    "\n",
    "    # the ul contains all the stories or articles found in each page\n",
    "    temp = driver.find_elements(By.XPATH, '//ul[@id=\"grid_thumbnail_stories\"]')\n",
    "\n",
    "    # all those with tag a under the previous ul with the specified class contains the href\n",
    "    for each in temp[0].find_elements(By.XPATH, '//a[@class=\"story_link story\"]'):\n",
    "        link = each.get_attribute(\"href\")\n",
    "        if link not in gma_master:\n",
    "            gma_master.append(link)\n",
    "\n",
    "    # the GMA site needs scrolling to load the following pages\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    counter+=1\n",
    "\n",
    "    if counter==1001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PSkIvv3M1Ki"
   },
   "source": [
    "### GMA News Articles\n",
    "The links obtained in the previous code have been appended to gma_master. Thus, in this part, each of those links will be scraped to obtain the content needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSG53xRMMa1H"
   },
   "outputs": [],
   "source": [
    "# creating the DataFrame with the needed content\n",
    "gma_df = pd.DataFrame(columns=['Link', 'Author','Content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQ-PwDiMuLv8"
   },
   "source": [
    "The loop below is used to get all the necessary information out of each article. Since some information has different XPaths per article, we make use of the try and except functions. All the obtained information is then transferred to the dataframe named gma_df which will then be used in data pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqlze-KNMomM"
   },
   "outputs": [],
   "source": [
    "# to know how many articles have already been scraped\n",
    "scraped_count = 0\n",
    "\n",
    "for link in gma_master:\n",
    "    print (scraped_count, link)\n",
    "    driver.get(link)\n",
    "\n",
    "    time.sleep (pause)\n",
    "\n",
    "    checker = True\n",
    "\n",
    "    # to obtain the actual content of each article, we create a container for the paragraphs\n",
    "    paragraphs = []\n",
    "\n",
    "    # different articles have different XPaths for the whole content of the article\n",
    "    while checker:\n",
    "        try:\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"story_main\"]')\n",
    "        except:\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"article-body\"]')\n",
    "\n",
    "    # the following codes are needed to get only the text of those with tag p directly under content\n",
    "        pars = content.find_elements(By.XPATH, 'p')\n",
    "        for par in pars:\n",
    "            text = par.text.strip()\n",
    "            if text:\n",
    "                paragraphs.append(text)\n",
    "    # since there are several paragraphs in one article, there is a need to append\n",
    "        concat_pars = ' '.join(paragraphs)\n",
    "\n",
    "    # to get the author:\n",
    "        try:\n",
    "            author_elements = content.find_elements(By.XPATH, '//div[@class=\"main-byline\"]')\n",
    "            author = [element.text for element in author_elements]\n",
    "        except:\n",
    "            try:\n",
    "                author_elements = content.find_elements(By.XPATH, '//div[@class=\"article-author\"]')\n",
    "                author = [element.text for element in author_elements]\n",
    "            except:\n",
    "                author = [\"Author not found\"]\n",
    "\n",
    "        checker = False\n",
    "        scraped_count += 1\n",
    "\n",
    "    # appending all the scraped content into the DataFrame\n",
    "    gma_df = gma_df.append({'Link': link,'Author':author, 'Content': concat_pars}, ignore_index=True)\n",
    "    # storing everything into a CSV file to prevent loss\n",
    "    gma_df.to_csv('gma_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_iJDbtlNbTJ"
   },
   "source": [
    "## Web Scraping `Rappler`\n",
    "\n",
    "The method used in scraping GMA is done with Rappler. Similarly, we obtain the links first before scraping each article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9K0oMVvNd4y"
   },
   "source": [
    "### Rappler Pages\n",
    "> In scraping Rappler's pages, there was a need to change the last part of the URL instead of scrolling (which was done on GMA's). Through the loop below, we are able to navugate through Rappler's different pages and use the appropriate XPaths after checking the site's HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FabXQG4PPVBp"
   },
   "source": [
    "The following variables will be used:\n",
    "\n",
    "* rap_master : container for the links\n",
    "* checker : will be used for the following loops to run\n",
    "* counter : used to limit the number of scraped pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKTcSGPSNlvb"
   },
   "outputs": [],
   "source": [
    "rap_master = []\n",
    "checker = True\n",
    "counter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzyubl7RPtKt"
   },
   "source": [
    "The following functions will be used to locate the links:\n",
    "* '.find_elements' : for locating multiple elements (in this case, articles)\n",
    "* '.get_attribute' : for retrieving a specific attribute inside the element (in this case, for getting the link found in each element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ARVatQANk_e"
   },
   "outputs": [],
   "source": [
    "while checker:\n",
    "    url_rap=f\"https://www.rappler.com/latest/page/{counter}/\"\n",
    "    print ('Scraping', url_rap)\n",
    "    driver.get(url_rap)\n",
    "\n",
    "    time.sleep(pause)\n",
    "\n",
    "    # contains all the articles\n",
    "    temp = driver.find_elements(By.XPATH, '//main[@id=\"primary\"]')\n",
    "\n",
    "    # to get all the links in the current page\n",
    "    for each in temp[0].find_elements(By.XPATH, '//article//h2//a'):\n",
    "        link = each.get_attribute(\"href\")\n",
    "        if link not in rap_master:\n",
    "            rap_master.append(link)\n",
    "\n",
    "    time.sleep(pause)\n",
    "    # necessary to change the url\n",
    "    counter+=1\n",
    "\n",
    "    if counter==1200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z71CmgUFNfQw"
   },
   "source": [
    "### Rappler Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCjdspMlP4Th"
   },
   "source": [
    "We start by  reating the DataFrame that will contain all the content from each article from Rappler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qz7WOdmdSlDL"
   },
   "outputs": [],
   "source": [
    "rap_df = pd.DataFrame(columns=['Link', 'Author','Content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCYBxfBJvEED"
   },
   "source": [
    "The loop below is used to get all the necessary information out of each article. Since some information has different XPaths per article, we make use of the try and except functions. All the obtained information is then transferred to the dataframe named rap_df which will then be used in data pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mr_1rY5-NcMx"
   },
   "outputs": [],
   "source": [
    "# to know how many articles have already been scraped\n",
    "scraped_count = 0\n",
    "\n",
    "for link in rap_master:\n",
    "    print (scraped_count, link)\n",
    "    driver.get(link)\n",
    "\n",
    "    time.sleep (pause)\n",
    "\n",
    "    checker = True\n",
    "    paragraphs = []\n",
    "    while checker:\n",
    "        # for main content\n",
    "        content = driver.find_element(By.XPATH, '//div[@class=\"post-single__content entry-content\"]')\n",
    "        pars = content.find_elements(By.XPATH, 'p')\n",
    "        for par in pars:\n",
    "            text = par.text.strip()\n",
    "            if text:\n",
    "                paragraphs.append(text)\n",
    "        # again, all the paragraphs have to be appended since not all the paragraphs are contained in a single tag\n",
    "        concat_pars = ' '.join(paragraphs)\n",
    "        checker = False\n",
    "\n",
    "        # to obtain the author, the following XPath is used:\n",
    "        try:\n",
    "            author_elements = content.find_elements(By.XPATH, '//a[@class=\"post-single__author\"]')\n",
    "            author = [element.text for element in author_elements]\n",
    "        except:\n",
    "            author = [\"Author not found\"]\n",
    "\n",
    "        scraped_count += 1\n",
    "\n",
    "        # appending all the obtained content to the DataFrame\n",
    "    rap_df = rap_df.append({'Link': link,'Author':author, 'Content': concat_pars}, ignore_index=True)\n",
    "        # storing to prevent loss\n",
    "    rap_df.to_csv('rap_dataframe.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
