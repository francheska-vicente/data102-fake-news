{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection Part 1**: Web Scraping <u>Real News</u>\n",
    "\n",
    "This notebook demonstrates the first part of this project's data collection which is web scraping real news from well-known and considered as reliable news sites in the Philippines. \n",
    "\n",
    "The **news sites involved** in collecting real news are the following: \n",
    "1. [`GMA News`](https://www.gmanetwork.com/news/topstories/)\n",
    "2. [`Rappler`](https://www.rappler.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-a45YrMKlE2"
   },
   "source": [
    "# Importing Libraries\n",
    "To start, we will be importing libraries that would help us perform web scraping and data processing properly.\n",
    "\n",
    "## Basic Libraries\n",
    "*  [`time`](https://docs.python.org/3/library/time.html): provides functions for handling time related tasks \n",
    "> this allows us to enable pauses for the loops that we will run later on\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/) : has functions for data analysis and manipulation\n",
    "> this allows us to obtain and organize all scraped data in a format that can be used for Exploratory Data Analysis (EDA) and for creating a model for fake news detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sP_GeHWhKrpV"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yz_1GVS2MHH8"
   },
   "source": [
    "## Web Scraping Library: [`Selenium`](https://selenium-python.readthedocs.io/index.html)\n",
    "> The [`Selenium`](https://selenium-python.readthedocs.io/index.html) library will be used for  web scraping GMA and Rappler since navigation through multiple pages will be performed on both sites.\n",
    "\n",
    "*    `from selenium import webdriver` enables automation of web browser\n",
    "*    `from selenium.webdriver.common.keys import Keys` enables simulation of keyboard keys\n",
    "*   `from selenium.webdriver.common.by import By` helps with locating elements in the webpage through XPath\n",
    "*   `from selenium.webdriver.chrome.options import Options` enables us to specify certain preferences when initializing the WebDriver, aids in preventing multiple windows from initializing when performing the loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWwceB84JjQl"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MepOlkUMMXdW"
   },
   "source": [
    "# Preparing [`Chrome WebDriver`](https://chromedriver.chromium.org/downloads) for Selenium Web Scraping\n",
    "\n",
    "Since the needed libraries have already been imported, we will now **set up our Chrome WebDriver** before we start web scraping using the Selenium library. \n",
    "\n",
    "* [`Chrome WebDriver`](https://chromedriver.chromium.org/getting-started) is a driver used for the Selenium WebDriver to have control on the Chrome window during web scraping.\n",
    "\n",
    "## 1. To start setting up the driver, download the drive from the [`ChromeDriver site`](https://chromedriver.chromium.org/downloads). \n",
    "It is recommended to put the downloaded file in a file location that you would be easy to locate it or in the same file location as the project. This is so that performing the next step would be easy for you.\n",
    "\n",
    "## 2. Set the driver path \n",
    "This part necessitates the user to specify their own driver path in their device and store it in a variable, in this case, the driver path will be stored in the `driver_path` variable. This will enable scraping through initializing the Chrome WebDriver in the later step of the set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-3AR-bmMcSn"
   },
   "outputs": [],
   "source": [
    "# `driver_path` : contains the path where the WebDriver is found\n",
    "driver_path=\"/Users/ajmarcelo/Downloads/chromedriver_mac64/chromedriver\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jnpth1-DOLW4"
   },
   "source": [
    "## 3. Specify Chrome Preferences\n",
    "In this step, we will  we will specify opting to disable opening a visible GUI each time the subsequent loops are ran. With the help of the [`Options()`](https://chromedriver.chromium.org/capabilities), we would be able to specify our chrome preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIDhJqAjnYrI"
   },
   "outputs": [],
   "source": [
    "# `chrome_options` : handles the  specified chrome preferences\n",
    "chrome_options = Options()\n",
    "\n",
    "#  \"--headless=new\" : disables opening new windows or any GUI even if the browser is running\n",
    "chrome_options.add_argument(\"--headless=new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Initialize Chrome Driver\n",
    "\n",
    "Aas for the last step for setting up the Chrome Driver, we will be needing the driver path and chrome preferences we specified from Steps 2 and 3. \n",
    "\n",
    "To recall, we have stored these values to the following variables:\n",
    "* `driver_path` : contains the file path where your Chrome WebDriver is found\n",
    "* `chrome_options` : contains the chrome preferences we specified from Step 3\n",
    "\n",
    "Through inserting these two variables into the [`webdriver.Chrome()`](https://sites.google.com/chromium.org/driver/capabilities?authuser=0), we will now initialize the driver to be used later for web scraping and store it to a variable named `driver`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `driver`: contains the function for setting the Chrome's behavior once the selenium starts web scraping\n",
    "driver = webdriver.Chrome(driver_path, chrome_options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping News Sites\n",
    "\n",
    "This section is divided into two major parts which are: (1) `Web Scraping GMA News` and (2) `Web Scraping Rappler`. Both web scraping will be performed with the help of **Selenium**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhPQv06wMRrQ"
   },
   "source": [
    "## **Part 1.1.** `GMA News` Data\n",
    "GMA is widely known in the Philippines for **disseminating news** and **producing entertainment shows**.\n",
    "In a Reuters study in 2021, GMA Network was found to have the **highest news brand trust score** (Gonzales, 2021). The network also ranked **first in having the widest reach online** through their site GMA News Online (Chua, 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIzYtfjZM7ce"
   },
   "source": [
    "### Web Scraping GMA News Pages\n",
    "This section will demonstrate the process of web scraping the news articles from the GMA News site. Specifically, the **GMA News page containing a list of its articles** will be web scraped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbUGYqzzthgv"
   },
   "source": [
    "The url below will be scraped - notably, it needs scrolling to load subsequent pages. In the said page, all the articles may be seen; thus, we start by getting the **links of each article** first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `gma_url` : handles the URL of the GMA News site page that contains the articles\n",
    "gma_url = f\"https://www.gmanetwork.com/news/archives/topstories/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the variables declared in the cell below will be used in the web scraping process. Kindly refer to the comments on the purpose of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Izen0VCRM9Wc"
   },
   "outputs": [],
   "source": [
    "# `counter` : tracks the count of articles that are being scraped from the site\n",
    "#           : will be used to limit the number of scraped pages (limit = 1001)\n",
    "# `gma_master` : contains the GMA news article links that will be retrieved through scraping\n",
    "# `pause` : holds the number of delay in seconds between requests during the whole process of web scraping\n",
    "\n",
    "counter = 1\n",
    "gma_master = []\n",
    "pause = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be using the `.get()` to load and access the full GMA News site page. To check what the function actually does, you may *uncomment and run the cell below*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the URL we want to load and access, we will insert the GMA News URL link we have set in one of the previous cells into the `.get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Px62B5tZnWbK"
   },
   "outputs": [],
   "source": [
    "# `gma_url` : handles the URL of the GMA News site page that contains the articles\n",
    "driver.get(gma_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcqcp34PPqIV"
   },
   "source": [
    "In the following code, [`XPath`](https://www.guru99.com/xpath-selenium.html) is used to **locate and obtain the elements from the HTML page**. The loop below will be ran as the site scrolls.\n",
    "\n",
    "* [`.find_elements()`](https://selenium-python.readthedocs.io/locating-elements.html) : for locating multiple elements (in this case, articles)\n",
    "* [`.get_attribute()`](https://selenium-python.readthedocs.io/api.html?highlight=.get_attribute#selenium.webdriver.remote.webelement.WebElement.get_attribute) : for retrieving a specific attribute inside the element (in this case, for getting the link found in each element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1s4PNLCNBGs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `checker` : `True` until it reaches the quantity limit of the scraped pages (limit = 1001)\n",
    "# `counter` : tracks the number of scraped GMA news pages\n",
    "#           : will be used to limit the number of scraped pages (limit = 1001)\n",
    "# `gma_master` : contains the GMA news article links that will be retrieved through scraping\n",
    "# `pause` : holds the number of delay in seconds between requests during the whole process of web scraping\n",
    "\n",
    "checker = True\n",
    "\n",
    "while checker:\n",
    "    \n",
    "    # retrieving the url that we are currently accessing\n",
    "    url = driver.current_url \n",
    "    print (url)\n",
    "\n",
    "    # applying a 3-second delay between requests\n",
    "    time.sleep(pause) \n",
    "\n",
    "    # the `ul` contains all the stories or articles found in each page\n",
    "    temp = driver.find_elements(By.XPATH, '//ul[@id=\"grid_thumbnail_stories\"]')\n",
    "\n",
    "    # all those with tag `a` under the previous `ul` with the specified class contains the href\n",
    "    for each in temp[0].find_elements(By.XPATH, '//a[@class=\"story_link story\"]'):\n",
    "        # retrieves the href attribute or the link from the tag `a` in the HTML\n",
    "        link = each.get_attribute(\"href\")\n",
    "        # checks if the retrieved link already exists in the `gma_master` \n",
    "        if link not in gma_master:\n",
    "            # appends the link to `gma_master` if the link is not on the list yet\n",
    "            gma_master.append(link)\n",
    "\n",
    "    # for scrolling in the GMA site to load the following pages\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # counter increments every after all article links in a GMA News site page is web scraped\n",
    "    counter+=1\n",
    "    \n",
    "    # checks if the limit number of pages (1001) to be web scraped is already reached\n",
    "    if counter == 1001:\n",
    "        # turns `checker` to False if it reaches the limit, stopping the web scraping loop\n",
    "        checker = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PSkIvv3M1Ki"
   },
   "source": [
    "### Processing the Web Scraped GMA News Article Links\n",
    "\n",
    "The links obtained in the previous code have been appended to `gma_master`. Thus, in this part, each of those article links will be scraped to obtain the article content needed.\n",
    "\n",
    "To start, we will be creating a pandas DataFrame using [`DataFrame()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). The dataframe will be created with the following columns, representing the article information we will be retrieving from each of the articles: \n",
    "1. `Link` - article link\n",
    "2. `Author` - author/s of the article\n",
    "3. `Content` - the whole article itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSG53xRMMa1H"
   },
   "outputs": [],
   "source": [
    "# `gma_df` : a DataFrame for storing the GMA news articles and its specific details\n",
    "\n",
    "gma_df = pd.DataFrame(columns=['Link', 'Author','Content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQ-PwDiMuLv8"
   },
   "source": [
    "Next, the loop below is used to **get all the necessary information out of each article**. \n",
    "\n",
    "Since some information has <u>different XPaths per article</u>, we make use of the *try and except* functions. \n",
    "\n",
    "All the obtained information is then transferred to the dataframe named `gma_df` which will then be used in data pre-processing later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqlze-KNMomM"
   },
   "outputs": [],
   "source": [
    "# `scraped_count` : counter for articles that have already been scraped\n",
    "scraped_count = 0\n",
    "\n",
    "# exploring each article link that was stored in `gma_master`\n",
    "for link in gma_master:\n",
    "    print (scraped_count, link)\n",
    "    \n",
    "    # accesses the specific page of a GMA article\n",
    "    driver.get(link)\n",
    "    \n",
    "    # applying the 3-second delay between requests\n",
    "    time.sleep (pause)\n",
    "\n",
    "    checker = True\n",
    "\n",
    "    # to obtain the actual content of each article, we create a container for the paragraphs\n",
    "    paragraphs = []\n",
    "\n",
    "    # different articles have different XPaths for the whole content of the article\n",
    "    while checker:\n",
    "        try:\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"story_main\"]')\n",
    "        except:\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"article-body\"]')\n",
    "\n",
    "    # the following codes are needed to get only the text of those with tag `p` directly under content\n",
    "        pars = content.find_elements(By.XPATH, 'p')\n",
    "        for par in pars:\n",
    "            text = par.text.strip()\n",
    "            if text:\n",
    "                paragraphs.append(text)\n",
    "    \n",
    "    # since there are several paragraphs in one article, there is a need to append the paragraphs\n",
    "        concat_pars = ' '.join(paragraphs)\n",
    "\n",
    "    # to get the author:\n",
    "        try:\n",
    "            author_elements = content.find_elements(By.XPATH, '//div[@class=\"main-byline\"]')\n",
    "            author = [element.text for element in author_elements]\n",
    "        except:\n",
    "            try:\n",
    "                author_elements = content.find_elements(By.XPATH, '//div[@class=\"article-author\"]')\n",
    "                author = [element.text for element in author_elements]\n",
    "            except:\n",
    "                author = [\"Author not found\"]\n",
    "\n",
    "        checker = False\n",
    "        \n",
    "    # increments every after an article is web scraped\n",
    "    scraped_count += 1\n",
    "\n",
    "    # appending all the scraped content into the DataFrame\n",
    "    gma_df = gma_df.append({'Link': link,'Author':author, 'Content': concat_pars}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the GMA News Article Data to CSV\n",
    "With the help of pandas' [`to_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html), we will be exporting the web scraped articles stored in `gma_df` to CSV with the filename, **`gma_dataframe.csv`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sP_GeHWhKrpV"
   },
   "outputs": [],
   "source": [
    "gma_df.to_csv('gma_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_iJDbtlNbTJ"
   },
   "source": [
    "## Part 1.2. `Rappler` News Data\n",
    "\n",
    "Rappler is said to have an **extensive audience** despite it being controversial due to the Duterte administration (Chua, 2023). The online news website focuses on **investigative journalism**, and has become one of the **most notable news sources** in the Philippines (Britannica, n.d.).\n",
    "\n",
    "The same process used in web scraping the GMA News data will also be done with Rappler. Similarly, we obtain the links first before scraping each article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9K0oMVvNd4y"
   },
   "source": [
    "### Web Scraping Rappler Pages\n",
    "\n",
    "This section will demonstrate the process of web scraping the news articles from the Rappler site. In scraping Rappler's pages, there was a need to <u>change the last part of the URL</u> instead of scrolling (which was done on GMA's). Through the loop below, we are able to **navigate through Rappler's different pages** and use the <u>appropriate XPaths after checking the site's HTML</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKTcSGPSNlvb"
   },
   "outputs": [],
   "source": [
    "# `rap_master` : contains the Rappler article links that will be retrieved through scraping\n",
    "# `checker` : True` until it reaches the quantity limit of the scraped pages (limit = 1200)\n",
    "# `counter` : tracks the number of scraped Rappler pages\n",
    "#           : will be used to limit the number of scraped pages (limit = 1200)\n",
    "rap_master = []\n",
    "checker = True\n",
    "counter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzyubl7RPtKt"
   },
   "source": [
    "Similar with scraping the GMA News site,  [`XPath`](https://www.guru99.com/xpath-selenium.html) is used to **locate and obtain the elements from the HTML page**. To recall, these same functions will be used to locate the links:\n",
    "* [`.find_elements()`](https://selenium-python.readthedocs.io/locating-elements.html) : for locating multiple elements (in this case, articles)\n",
    "* [`.get_attribute()`](https://selenium-python.readthedocs.io/api.html?highlight=.get_attribute#selenium.webdriver.remote.webelement.WebElement.get_attribute) : for retrieving a specific attribute inside the element (in this case, for getting the link found in each element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ARVatQANk_e",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## `checker` : `True` until it reaches the quantity limit of the scraped pages (limit = 1200)\n",
    "# `counter` : tracks the number of scraped Rappler pages\n",
    "#           : will be used to limit the number of scraped pages (limit = 1200)\n",
    "#           : indicator to change the url or the site page number to be web scraped next\n",
    "# `rap_master` : contains the Rappler article links that will be retrieved through scraping\n",
    "# `pause` : holds the number of delay in seconds between requests during the whole process of web scraping\n",
    "\n",
    "while checker:\n",
    "    # acessing the Rappler page containing list of articles to be web scraped later\n",
    "    url_rap=f\"https://www.rappler.com/latest/page/{counter}/\"\n",
    "    print ('Scraping', url_rap)\n",
    "    driver.get(url_rap)\n",
    "    \n",
    "    # applying the 3-second delay\n",
    "    time.sleep(pause)\n",
    "\n",
    "    # contains all the article links\n",
    "    temp = driver.find_elements(By.XPATH, '//main[@id=\"primary\"]')\n",
    "\n",
    "    # to get all the links in the current page through the href attribute of each tag `a` in the page\n",
    "    for each in temp[0].find_elements(By.XPATH, '//article//h2//a'):\n",
    "        link = each.get_attribute(\"href\")\n",
    "        if link not in rap_master:\n",
    "            rap_master.append(link)\n",
    "    \n",
    "    # applying the 3-second delay \n",
    "    time.sleep(pause)\n",
    "    \n",
    "    # counter increments every after a Rappler page is web scraped\n",
    "    # necessary, this is also the indicator to change the url or the page to be web scraped next\n",
    "    counter+=1\n",
    "\n",
    "    # checks if the limit number of pages (1200) to be web scraped is already reached\n",
    "    if counter == 1200:\n",
    "        # turns `checker` to False if it reaches the limit, stopping the web scraping loop\n",
    "        checker = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z71CmgUFNfQw"
   },
   "source": [
    "### Processing the Web Scraped Rappler Article Links\n",
    "\n",
    "Similar to the processing of GMA News' article links, we start by creating the DataFrame that will contain the necessary information from each article from Rappler. This time, we will store the article information to the `rap_df` pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qz7WOdmdSlDL"
   },
   "outputs": [],
   "source": [
    "# `rap_df` : a DataFrame for storing the Rappler articles and its specific details\n",
    "\n",
    "rap_df = pd.DataFrame(columns=['Link', 'Author','Content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCYBxfBJvEED"
   },
   "source": [
    "From this, the loop below will be used **to extract the necessary information from each article**. \n",
    "\n",
    "Since some information has <u>distinct XPaths per article</u>, we will be using the *try and except* functions for a smooth process of web scraping. \n",
    "\n",
    "After all of the web scraping processes per article, the gathered article information will then be stored to `rap_df` and will be used in data pre-processing later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mr_1rY5-NcMx"
   },
   "outputs": [],
   "source": [
    "# `scraped_count` : counter for articles that have already been scraped\n",
    "scraped_count = 0\n",
    "\n",
    "# exploring each article link that was stored in `rap_master`\n",
    "for link in rap_master:\n",
    "    print (scraped_count, link)\n",
    "    \n",
    "    # accesses the specific page of a Rappler article\n",
    "    driver.get(link)\n",
    "    \n",
    "    # applying the 3-second delay between requests\n",
    "    time.sleep (pause)\n",
    "\n",
    "    checker = True\n",
    "    \n",
    "    # to obtain the actual content of each article, we create a container for the paragraphs\n",
    "    paragraphs = []\n",
    "    \n",
    "    while checker:\n",
    "        # for main content\n",
    "        content = driver.find_element(By.XPATH, '//div[@class=\"post-single__content entry-content\"]')\n",
    "        pars = content.find_elements(By.XPATH, 'p')\n",
    "        for par in pars:\n",
    "            text = par.text.strip()\n",
    "            if text:\n",
    "                paragraphs.append(text)\n",
    "        \n",
    "        # again, all the paragraphs have to be appended since not all the paragraphs are contained in a single tag\n",
    "        concat_pars = ' '.join(paragraphs)\n",
    "\n",
    "        # to obtain the author, the following XPath is used:\n",
    "        try:\n",
    "            author_elements = content.find_elements(By.XPATH, '//a[@class=\"post-single__author\"]')\n",
    "            author = [element.text for element in author_elements]\n",
    "        except:\n",
    "            author = [\"Author not found\"]\n",
    "        \n",
    "        checker = False\n",
    "    \n",
    "    # increments every after an article is web scraped \n",
    "    scraped_count += 1\n",
    "\n",
    "    # appending all the obtained content to the DataFrame\n",
    "    rap_df = rap_df.append({'Link': link,'Author':author, 'Content': concat_pars}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Rappler News Article Data to CSV\n",
    "Using pandas' [`to_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html), we will export the web scraped articles stored in `rap_df` to CSV with **`rap_dataframe.csv`** as its filename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rap_df.to_csv('rap_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "Britannica. (n.d.) *Rappler*. https://www.britannica.com/topic/Rappler\n",
    "\n",
    "Gonzales, G. (2021). *Trust in news from social media decreases, ABS-CBN reach tumbles in 2021 Reuters study*.\n",
    "https://www.rappler.com/technology/social-media-news-trust-decreases-filipinos-2021-reuters-digital-news-report/\n",
    "\n",
    "Chua, Y. (2021). *Pandemic increases trust in news among Filipinos - digital news report 2021*. \n",
    "https://news.abs-cbn.com/spotlight/06/23/21/internet-use-philippines-digital-news-2021\n",
    "\n",
    "Chua, Y. (2023). *Philippines*.\n",
    "https://reutersinstitute.politics.ox.ac.uk/digital-news-report/2023/philippines"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
