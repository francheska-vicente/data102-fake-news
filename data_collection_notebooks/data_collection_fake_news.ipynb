{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection Part 2**: Web Scraping <u>Fake News</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Library: BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Fake News Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "dptyHpHR2nSV",
    "outputId": "d991217f-92e8-47aa-b945-94a7ca87c24d"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-815900eca521>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Get the preview description of the article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entry'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mpreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# This Web Scraper makes use of BeautifulSoup to display various information about fake news articles from \"akoy-pilipino.blogspot.com\".\n",
    "\n",
    "# URL of the website you want to scrape\n",
    "url = 'http://akoy-pilipino.blogspot.com/search/label/Local%20News'\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the HTML elements that contain the fake news articles\n",
    "articles = soup.find_all(\"article\")\n",
    "\n",
    "# Create empty lists to store the data\n",
    "titles = []\n",
    "times = []\n",
    "authors = []\n",
    "previews = []\n",
    "\n",
    "# Extract information from each article\n",
    "for article in articles:\n",
    "    # Get the title of the article\n",
    "    title = article.find('h2', class_='post-title entry-title').text.strip()\n",
    "    titles.append(title)\n",
    "\n",
    "    # Get the publication date of the article\n",
    "    time = article.find('div', id='meta-post').text.strip()\n",
    "    times.append(time)\n",
    "\n",
    "    # Get the preview description of the article\n",
    "    preview = article.find('div', class_='entry').text.strip()\n",
    "    previews.append(preview)\n",
    "\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data_ap = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Date Posted': times,\n",
    "    'Description': previews\n",
    "})\n",
    "\n",
    "# Print the data\n",
    "print(data_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xN-ITmKG3mdG",
    "outputId": "b5ee5a12-f4cf-427c-db6b-c0a6e1a8ea04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Striking Metro grocery store worker says she ‘...   \n",
      "1  Unemployment inches up to 5.5% in July with sp...   \n",
      "2  Your latest questions about Bill C-18 and the ...   \n",
      "3  Stock markets slump as rating agency Fitch dow...   \n",
      "4  National Bank buys Silicon Valley Bank’s Canad...   \n",
      "5  Tupperware warned it might go bust — but its s...   \n",
      "6  Meta permanently ending news availability on i...   \n",
      "7  ‘Barbenheimer’ made this July the best one eve...   \n",
      "8  These Canadian companies switched to a 4-day w...   \n",
      "9  This man had a ‘poor’ credit rating because of...   \n",
      "\n",
      "                  Date Posted  \\\n",
      "0   1 day ago\\nBusiness World   \n",
      "1   1 day ago\\nBusiness World   \n",
      "2  2 days ago\\nBusiness World   \n",
      "3  3 days ago\\nBusiness World   \n",
      "4  3 days ago\\nBusiness World   \n",
      "5  3 days ago\\nBusiness World   \n",
      "6  4 days ago\\nBusiness World   \n",
      "7  4 days ago\\nBusiness World   \n",
      "8  5 days ago\\nBusiness World   \n",
      "9  5 days ago\\nBusiness World   \n",
      "\n",
      "                                         Description  \n",
      "0  More than 3,000 workers at 27 Metro grocery st...  \n",
      "1  There was little change to Canada's job market...  \n",
      "2  All Facebook and Instagram users in Canada won...  \n",
      "3  Wall Street fell on Wednesday after a move by ...  \n",
      "4  National Bank of Canada says it will acquire t...  \n",
      "5  Warning that you might go out of business isn'...  \n",
      "6  Social media giant Meta says it has officially...  \n",
      "7  Cineplex Inc. says it saw its highest July box...  \n",
      "8  A new study involving 41 companies from North ...  \n",
      "9  Velimir Drecun is warning other Meridian Credi...  \n"
     ]
    }
   ],
   "source": [
    "# This Web Scraper makes use of BeautifulSoup to display various information about fake news articles from \"https://www.maharlikanews.com/\".\n",
    "\n",
    "\n",
    "# URL of the website you want to scrape\n",
    "url = 'https://www.maharlikanews.com/category/world-of-business/'\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the HTML elements that contain the fake news articles\n",
    "articles = soup.find_all(\"article\")\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Create empty lists to store the data\n",
    "titles = []\n",
    "times = []\n",
    "authors = []\n",
    "previews = []\n",
    "# Extract information from each article\n",
    "for article in articles:\n",
    "    # Get the title of the article\n",
    "    title = article.find('h2', class_='post-box-title').text.strip()\n",
    "    titles.append(title)\n",
    "\n",
    "    # Get the publication date of the article\n",
    "    time = article.find('p', class_='post-meta').text.strip()\n",
    "    times.append(time)\n",
    "\n",
    "    # Get the preview description of the article\n",
    "    preview = article.find('div', class_='entry').text.strip()\n",
    "    previews.append(preview)\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data_maharlika = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Date Posted': times,\n",
    "    'Description': previews\n",
    "})\n",
    "\n",
    "# Print the data\n",
    "print(data_maharlika)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kJ_OxIk83nU"
   },
   "outputs": [],
   "source": [
    "# This Web Scraper makes use of BeautifulSoup to display various information about fake news articles from \"akoy-pilipino.blogspot.com\".\n",
    "\n",
    "\n",
    "# URL of the website you want to scrape\n",
    "url = 'http://akoy-pilipino.blogspot.com/search/label/International%20News'\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the HTML elements that contain the fake news articles\n",
    "articles = soup.find_all(\"article\")\n",
    "\n",
    "# Create empty lists to store the data\n",
    "titles = []\n",
    "times = []\n",
    "authors = []\n",
    "previews = []\n",
    "# Extract information from each article\n",
    "for article in articles:\n",
    "    # Get the title of the article\n",
    "    title = article.find('h2', class_='post-title entry-title').text.strip()\n",
    "    titles.append(title)\n",
    "\n",
    "    # Get the publication date of the article\n",
    "    time = article.find('div', id='meta-post').text.strip()\n",
    "    times.append(time)\n",
    "\n",
    "    # Get the preview description of the article\n",
    "    preview = article.find('div', class_='resumo').text.strip()\n",
    "    previews.append(preview)\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data_ap2 = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Date Posted': times,\n",
    "    'Description': previews\n",
    "})\n",
    "\n",
    "# Print the data\n",
    "print(data_ap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9X-zaHRCnYE"
   },
   "outputs": [],
   "source": [
    "# This Web Scraper makes use of BeautifulSoup to display various information about fake news articles from \"https://www.maharlikanews.com/\".\n",
    "\n",
    "\n",
    "# URL of the website you want to scrape\n",
    "url = 'https://www.maharlikanews.com/category/technology/'\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the HTML elements that contain the fake news articles\n",
    "articles = soup.find_all(\"article\")\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Create empty lists to store the data\n",
    "titles = []\n",
    "times = []\n",
    "authors = []\n",
    "previews = []\n",
    "# Extract information from each article\n",
    "for article in articles:\n",
    "    # Get the title of the article\n",
    "    title = article.find('h2', class_='post-box-title').text.strip()\n",
    "    titles.append(title)\n",
    "\n",
    "    # Get the publication date of the article\n",
    "    time = article.find('p', class_='post-meta').text.strip()\n",
    "    times.append(time)\n",
    "\n",
    "    # Get the preview description of the article\n",
    "    preview = article.find('div', class_='entry').text.strip()\n",
    "    previews.append(preview)\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data_maharlika3 = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Date Posted': times,\n",
    "    'Description': previews\n",
    "})\n",
    "\n",
    "# Print the data\n",
    "print(data_maharlika3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tf5WXcI6DQgE"
   },
   "outputs": [],
   "source": [
    "# This Web Scraper makes use of BeautifulSoup to display various information about fake news articles from \"https://www.maharlikanews.com/\".\n",
    "\n",
    "\n",
    "# URL of the website you want to scrape\n",
    "url = 'https://www.maharlikanews.com/category/in-the-news/philippine-news/'\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the HTML elements that contain the fake news articles\n",
    "articles = soup.find_all(\"article\")\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Create empty lists to store the data\n",
    "titles = []\n",
    "times = []\n",
    "authors = []\n",
    "previews = []\n",
    "# Extract information from each article\n",
    "for article in articles:\n",
    "    # Get the title of the article\n",
    "    title = article.find('h2', class_='post-box-title').text.strip()\n",
    "    titles.append(title)\n",
    "\n",
    "    # Get the publication date of the article\n",
    "    time = article.find('p', class_='post-meta').text.strip()\n",
    "    times.append(time)\n",
    "\n",
    "    # Get the preview description of the article\n",
    "    preview = article.find('div', class_='entry').text.strip()\n",
    "    previews.append(preview)\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data_maharlika4 = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Date Posted': times,\n",
    "    'Description': previews\n",
    "})\n",
    "\n",
    "# Print the data\n",
    "print(data_maharlika4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuVER4peC2kJ"
   },
   "outputs": [],
   "source": [
    "# This Web Scraper makes use of BeautifulSoup to display various information about fake news articles from \"https://www.maharlikanews.com/\".\n",
    "\n",
    "\n",
    "# URL of the website you want to scrape\n",
    "url = 'https://www.maharlikanews.com/category/in-the-news/the-world-news/'\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the HTML elements that contain the fake news articles\n",
    "articles = soup.find_all(\"article\")\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Create empty lists to store the data\n",
    "titles = []\n",
    "times = []\n",
    "authors = []\n",
    "previews = []\n",
    "# Extract information from each article\n",
    "for article in articles:\n",
    "    # Get the title of the article\n",
    "    title = article.find('h2', class_='post-box-title').text.strip()\n",
    "    titles.append(title)\n",
    "\n",
    "    # Get the publication date of the article\n",
    "    time = article.find('p', class_='post-meta').text.strip()\n",
    "    times.append(time)\n",
    "\n",
    "    # Get the preview description of the article\n",
    "    preview = article.find('div', class_='entry').text.strip()\n",
    "    previews.append(preview)\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data_maharlika5 = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Date Posted': times,\n",
    "    'Description': previews\n",
    "})\n",
    "\n",
    "# Print the data\n",
    "print(data_maharlika5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0IKr0308Toj"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([data_ap,data_maharlika,data_ap2,data_maharlika3,data_maharlika4, data_maharlika5]).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmIzefSCFBi2"
   },
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\AlyssaRianthaNavarro\\Desktop\\Personal\\fakenews_webscraping.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
