{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection Part 2**: Web Scraping <u>Fake News</u>\n",
    "\n",
    "This notebook demonstrates the first part of this project's data collection which is web scraping fake news from news sites in the Philippines. \n",
    "\n",
    "The **news sites involved** in collecting real news are the following: \n",
    "1. [`Ako'y Pilipino`](https://www.gmanetwork.com/news/topstories/)\n",
    "2. [`Maharlika News`](https://www.rappler.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "To start, we will be importing libraries that would help us perform web scraping and data processing properly.\n",
    "\n",
    "## Basic Libraries\n",
    "*  [`requests`](https://pypi.org/project/requests/): has functions for HTTP requests \n",
    "> allows us to make requests to a web page and access its contents\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/) : has functions for data analysis and manipulation\n",
    "> this allows us to obtain and organize all scraped data in a format that can be used for Exploratory Data Analysis (EDA) and for creating a model for fake news detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Library: [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "> The [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library will be used for web scraping fake news from Ako'y Filipino and Maharlika News since we will only be scraping 1 page at a time on both sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Fake News Sites\n",
    "\n",
    "In this section, there will be two primary sections which are (1) `Web Scraping Ako'y Pilipino News` and (2) `Web Scraping Maharlika News`. Both instances of web scraping will be executed using the assistance of the **BeautifulSoup** Library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1. Web Scraping Ako'y Pilipino Articles\n",
    "\n",
    "- [TO DO] Ako'y Pilipino Articles Definition\n",
    "- [TO DO] Screenshot of Ako'y Pilipino Articles Site Page that will be web scraped\n",
    "\n",
    "\n",
    "In web scraping fake news articles from Ako'y Pilipino, a function named `web_scrape_akoy_pilipino()` was created to web scrape articles from the **Local** and **International news** found in this site.\n",
    "\n",
    "`web_scrape_akoy_pilipino(url)`\n",
    "> To perform web scraping using this function, the <u>page url to be web scraped</u> would be inputted on the `url` parameter of the function. To understand the **process of web scraping the page through BeautifulSoup**, kindly refer to the <u>step-by-step comments</u> inside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def web_scrape_akoy_pilipino(url):\n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the HTML elements that contain the fake news articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # Create empty lists to store the data\n",
    "    titles = []\n",
    "    times = []\n",
    "    authors = []\n",
    "    contents = []\n",
    "\n",
    "    # Extract information from each article\n",
    "    for article in articles:\n",
    "        # Get the title of the article\n",
    "        title = article.find('h2', class_='post-title entry-title')\n",
    "        title = title.find('a', class_='').text.strip()\n",
    "        titles.append(title)\n",
    "\n",
    "        # Get the publication date of the article\n",
    "        time = article.find('div', id='meta-post').text.strip()\n",
    "        times.append(time)\n",
    "\n",
    "        # Get the preview description of the article\n",
    "        content = article.find('div', class_='entry').text.strip()\n",
    "        contents.append(content)\n",
    "\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    data = pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Date Posted': times,\n",
    "        'Content': contents\n",
    "    })\n",
    "\n",
    "    # Print the data\n",
    "    print(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping `Local` News from Ako'y Pilipino\n",
    "\n",
    "First, the `local_url` contains the page link of the **Local News tab** of the Ako'y Pilipino site. Then, `local_url` will be inputted as the URL parameter of the `web_scrape_akoy_pilipino()`. In this part of the collection, ***only local news articles*** will be retrieved from the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `local_url` : handles the URL of the International News tab of the Ako'y Pilipino\n",
    "# `data_ap` : handles the web scraped local articles data\n",
    "\n",
    "local_url = 'http://akoy-pilipino.blogspot.com/search/label/Local%20News'\n",
    "\n",
    "data_ap = web_scrape_akoy_pilipino(local_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping `International` News from Ako'y Pilipino\n",
    "\n",
    "Lastly, the `int_url` pertains to the webpage link of the **International News tab** on the Ako'y Pilipino website. Inserting the `int_url` into the URL parameter of the `web_scrape_akoy_pilipino()`, ***only articles under international news*** will be web scraped from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kJ_OxIk83nU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `int_url` : handles the URL of the International News tab of the Ako'y Pilipino\n",
    "# `data_ap_2` : handles the web scraped international articles data\n",
    "\n",
    "int_url = 'http://akoy-pilipino.blogspot.com/search/label/International%20News'\n",
    "\n",
    "data_ap_2 = web_scrape_akoy_pilipino(int_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2. Web Scraping `Maharlika News` Articles\n",
    "\n",
    "\n",
    "- [TO DO] Maharlika News Articles Definition\n",
    "- [TO DO] Screenshot of Maharlika News Articles Site Page that will be web scraped\n",
    "\n",
    "\n",
    "In this part of the collection, a function labeled `web_scrape_maharlika_news()` was formulated for the purpose of extracting fake news articles from Maharlika News through web scraping. Similar to `web_scrape_akoy_pilipino()` from the previous section, this function was designed to scrape articles present on the website. More specifically, we intend to scrape articles from the following categories within the website:<br>\n",
    ">**1.** World of Business<br>\n",
    ">**2.** Technology<br>\n",
    ">**3.** Philippine News<br>\n",
    ">**4.** World News<br>\n",
    "\n",
    "\n",
    "`web_scrape_maharlika_news(url)`\n",
    "> The <u>page url to be web scraped</u> would be inputted on the `url` parameter of the function to execute the web scraping. To understand the **process of web scraping the page through BeautifulSoup and through this function**, kindly read the <u>step-by-step comments</u> within the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrape_maharlika_news(url):\n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the HTML elements that contain the fake news articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "    \n",
    "    # Create empty lists to store the data\n",
    "    titles = []\n",
    "    times = []\n",
    "    authors = []\n",
    "    contents = []\n",
    "    \n",
    "    # Extract information from each article\n",
    "    for article in articles:\n",
    "        # Get the title of the article\n",
    "        title = article.find('h2', class_='post-box-title').text.strip()\n",
    "        titles.append(title)\n",
    "\n",
    "        # Get the publication date of the article\n",
    "        time = article.find('p', class_='post-meta').text.strip()\n",
    "        times.append(time)\n",
    "\n",
    "        # Get the preview description of the article\n",
    "        content = article.find('div', class_='entry').text.strip()\n",
    "        contents.append(content)\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    data = pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Date Posted': times,\n",
    "        'Content': contents\n",
    "    })\n",
    "\n",
    "    # Print the data\n",
    "    print(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping `Business` News from Maharlika News\n",
    "\n",
    "In this section, the `bus_url` will handle the page link of the **Business tab** of the Maharlika News site. Then, the  `bus_url` will be inputted to the function url parameter of the `web_scrape_maharlika_news()`. From this, ***only business news articles*** will be retrieved from the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xN-ITmKG3mdG",
    "outputId": "b5ee5a12-f4cf-427c-db6b-c0a6e1a8ea04",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `bus_url` : handles the URL of the Business tab of the Maharlika News\n",
    "# `data_maharlika` : handles the web scraped business articles data\n",
    "\n",
    "bus_url = 'https://www.maharlikanews.com/category/world-of-business/'\n",
    "\n",
    "data_maharlika = web_scrape_maharlika_news(bus_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping `Techonology` News from Maharlika News\n",
    "\n",
    "This time,  `tech_url` will handle the page link of the **Techonology tab** of the Maharlika News site and will be inputted into the the `web_scrape_maharlika_news()`. This would mean that this part of the scraping will ***only include techonology news articles*** from the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9X-zaHRCnYE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `tech_url` : handles the URL of the Technology tab of the Maharlika News\n",
    "# `data_maharlika_2` : handles the web scraped technology articles data\n",
    "\n",
    "tech_url = 'https://www.maharlikanews.com/category/technology/'\n",
    "\n",
    "data_maharlika_2 = web_scrape_maharlika_news(tech_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping `Philippine News` from Maharlika News\n",
    "\n",
    "Next, the page link of the **Philippine News under the Maharlika News tab** of the site will be stored in `ph_url`. From this, the `ph_url` will be passed to the `web_scrape_maharlika_news()` to ***only include Philippine news articles*** in web scraping the site specifically in this part of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tf5WXcI6DQgE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `ph_url` : handles the URL of the Philippines News tab of the Maharlika News\n",
    "# `data_maharlika_3` : handles the web scraped Philippine news articles data\n",
    "\n",
    "ph_url = 'https://www.maharlikanews.com/category/in-the-news/philippine-news/'\n",
    "\n",
    "data_maharlika_3 = web_scrape_maharlika_news(ph_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping `World News` from Maharlika News\n",
    "\n",
    "Finally, the `world_url` will handle the page link of the **World News under the Maharlika News tab** of the site. Then, by running the `web_scrape_maharlika_news()` function with the `world_url` value assigned to its URL parameter, ***only Philippine news articles*** will be retrieved from the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuVER4peC2kJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `world_url` : handles the URL of the Philippines News tab of the Maharlika News\n",
    "# `data_maharlika_4` : handles the web scraped Philippine news articles data\n",
    "\n",
    "world_url = 'https://www.maharlikanews.com/category/in-the-news/the-world-news/'\n",
    "\n",
    "data_maharlika_4 = web_scrape_maharlika_news(world_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Fake News Data \n",
    "\n",
    "Now that we have collected the fake news data from **Ako'y Pilipino** and **Maharlika News**, we will now proceed to <u>combining the data into one dataframe</u>. \n",
    "\n",
    "To combine the fake news data from both sites, we  will be utilizing the panda' [`concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) and make use of the pandas' [`reset_index()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) to fix the indexing of the combined fake news data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0IKr0308Toj"
   },
   "outputs": [],
   "source": [
    "fakenews_df = pd.concat([data_ap, data_maharlika,data_ap2,data_maharlika3,data_maharlika4, data_maharlika5]).reset_index(drop=True)\n",
    "fakenews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Combined Fake News Article Data to CSV\n",
    "With the help of pandas' [`to_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html), we will be exporting the web scraped fake news articles stored in `fakenews_df` to CSV with the filename, **`fakenews_dataframe.csv`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmIzefSCFBi2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fakenews_df.to_csv(\"fakenews_dataframe.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
